{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e1cd147",
   "metadata": {},
   "source": [
    "# Tensorflow Object Detection API and AWS Sagemaker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85592c17",
   "metadata": {},
   "source": [
    "In this notebook, you will train and evaluate different models using the [Tensorflow Object Detection API](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/) and [AWS Sagemaker](https://aws.amazon.com/sagemaker/). \n",
    "\n",
    "If you ever feel stuck, you can refer to this [tutorial](https://aws.amazon.com/blogs/machine-learning/training-and-deploying-models-using-tensorflow-2-with-the-object-detection-api-on-amazon-sagemaker/).\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We are using the [Waymo Open Dataset](https://waymo.com/open/) for this project. The dataset has already been exported using the tfrecords format. The files have been created following the format described [here](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#create-tensorflow-records). You can find data stored on [AWS S3](https://aws.amazon.com/s3/), AWS Object Storage. The images are saved with a resolution of 640x640."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc1d114",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install tensorflow_io sagemaker -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f55350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from framework import CustomFramework"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccde6fd1",
   "metadata": {},
   "source": [
    "Save the IAM role in a variable called `role`. This would be useful when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ab6b13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::738138308218:role/service-role/AmazonSageMaker-ExecutionRole-20250923T072516\n"
     ]
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae64e29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The train and val paths below are public S3 buckets created by Udacity for this project\n",
    "inputs = {'train': 's3://cd2688-object-detection-tf2/train/', \n",
    "          'val': 's3://cd2688-object-detection-tf2/val/'} \n",
    "\n",
    "# Insert path of a folder in your personal S3 bucket to store tensorboard logs.\n",
    "tensorboard_s3_prefix = 's3://object-detection-bucket-2/logs/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc16a825",
   "metadata": {},
   "source": [
    "## Container\n",
    "\n",
    "To train the model, you will first need to build a [docker](https://www.docker.com/) container with all the dependencies required by the TF Object Detection API. The code below does the following:\n",
    "* clone the Tensorflow models repository\n",
    "* get the exporter and training scripts from the repository\n",
    "* build the docker image and push it \n",
    "* print the container name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad5ac8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'docker/models' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# clone the repo and get the scripts\n",
    "git clone https://github.com/tensorflow/models.git docker/models\n",
    "\n",
    "# get model_main and exporter_main files from TF2 Object Detection GitHub repository\n",
    "cp docker/models/research/object_detection/exporter_main_v2.py source_dir \n",
    "cp docker/models/research/object_detection/model_main_tf2.py source_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2dab3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#14 7.497 Collecting protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 (from apache-beam->object-detection==0.1)\n",
      "#14 7.501   Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "#14 7.521 Collecting pydot<2,>=1.2.0 (from apache-beam->object-detection==0.1)\n",
      "#14 7.525   Downloading pydot-1.4.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "#14 7.572 Collecting redis<6,>=5.0.0 (from apache-beam->object-detection==0.1)\n",
      "#14 7.576   Downloading redis-5.3.1-py3-none-any.whl.metadata (9.2 kB)\n",
      "#14 7.628 Collecting requests<3.0.0,>=2.24.0 (from apache-beam->object-detection==0.1)\n",
      "#14 7.632   Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "#14 7.639 Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (4.5.0)\n",
      "#14 7.722 Collecting zstandard<1,>=0.18.0 (from apache-beam->object-detection==0.1)\n",
      "#14 7.728   Downloading zstandard-0.23.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "#14 7.865 Collecting pyarrow<17.0.0,>=3.0.0 (from apache-beam->object-detection==0.1)\n",
      "#14 7.871   Downloading pyarrow-16.1.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "#14 7.887 Collecting pyarrow-hotfix<1 (from apache-beam->object-detection==0.1)\n",
      "#14 7.893   Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "#14 7.914 Collecting cycler>=0.10.0 (from lvis->object-detection==0.1)\n",
      "#14 7.918   Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "#14 7.996 Collecting kiwisolver>=1.1.0 (from lvis->object-detection==0.1)\n",
      "#14 8.000   Downloading kiwisolver-1.4.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (6.3 kB)\n",
      "#14 8.076 Collecting opencv-python>=4.1.0.25 (from lvis->object-detection==0.1)\n",
      "#14 8.080   Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "#14 8.157 Collecting contourpy>=1.0.1 (from matplotlib->object-detection==0.1)\n",
      "#14 8.161   Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
      "#14 8.318 Collecting fonttools>=4.22.0 (from matplotlib->object-detection==0.1)\n",
      "#14 8.321   Downloading fonttools-4.57.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n",
      "#14 8.378 Collecting importlib-resources>=3.2.0 (from matplotlib->object-detection==0.1)\n",
      "#14 8.381   Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "#14 8.480 Collecting tensorflow-io-gcs-filesystem==0.34.0 (from tensorflow_io->object-detection==0.1)\n",
      "#14 8.487   Downloading tensorflow_io_gcs_filesystem-0.34.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
      "#14 8.516 Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.21.0)\n",
      "#14 8.533 Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 8.538   Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "#14 8.633 Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 8.636   Downloading google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "#14 8.672 Collecting uritemplate<5,>=3.0.1 (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 8.690   Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "#14 8.720 Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1)\n",
      "#14 8.723   Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "#14 8.735   Preparing metadata (setup.py): started\n",
      "#14 8.883   Preparing metadata (setup.py): finished with status 'done'\n",
      "#14 8.907 Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=3.2.0->matplotlib->object-detection==0.1) (3.15.0)\n",
      "#14 8.927 Collecting attrs>=22.2.0 (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1)\n",
      "#14 8.938   Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "#14 8.979 Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1)\n",
      "#14 8.982   Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "#14 8.998 Collecting pkgutil-resolve-name>=1.3.10 (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1)\n",
      "#14 9.001   Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl.metadata (624 bytes)\n",
      "#14 9.032 Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1)\n",
      "#14 9.035   Downloading referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "#14 9.316 Collecting rpds-py>=0.7.1 (from jsonschema<5.0.0,>=4.0.0->apache-beam->object-detection==0.1)\n",
      "#14 9.320   Downloading rpds_py-0.20.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "#14 9.357 Collecting bleach (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 9.361   Downloading bleach-6.1.0-py3-none-any.whl.metadata (30 kB)\n",
      "#14 9.369 Requirement already satisfied: certifi>=14.05.14 in /usr/lib/python3/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2019.11.28)\n",
      "#14 9.443 Collecting charset-normalizer (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 9.453   Downloading charset_normalizer-3.4.3-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
      "#14 9.461 Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.8)\n",
      "#14 9.478 Collecting python-slugify (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 9.482   Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "#14 9.489 Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (68.0.0)\n",
      "#14 9.519 Collecting text-unidecode (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 9.523   Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "#14 9.564 Collecting tqdm (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 9.567   Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "#14 9.635 Requirement already satisfied: urllib3>=1.15.1 in /usr/lib/python3/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.25.8)\n",
      "#14 9.635 Requirement already satisfied: webencodings in /usr/lib/python3/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (0.5.1)\n",
      "#14 9.737 Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam->object-detection==0.1)\n",
      "#14 9.742   Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "#14 9.777 Collecting PyJWT>=2.9.0 (from redis<6,>=5.0.0->apache-beam->object-detection==0.1)\n",
      "#14 9.782   Downloading PyJWT-2.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "#14 9.805 Collecting async-timeout>=4.0.3 (from redis<6,>=5.0.0->apache-beam->object-detection==0.1)\n",
      "#14 9.809   Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "#14 9.835 Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
      "#14 9.836 Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (23.5.26)\n",
      "#14 9.836 Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
      "#14 9.837 Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
      "#14 9.837 Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (3.9.0)\n",
      "#14 9.838 Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (16.0.0)\n",
      "#14 9.839 Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
      "#14 9.944 Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (2.13.0)\n",
      "#14 9.945 Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (2.13.0)\n",
      "#14 9.945 Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (2.3.0)\n",
      "#14 9.946 Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (1.15.0)\n",
      "#14 10.09 Collecting tf-keras>=2.14.1 (from tensorflow-hub>=0.6.0->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 10.09   Downloading tf_keras-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "#14 10.12 Collecting dm-tree~=0.1.1 (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 10.13   Downloading dm_tree-0.1.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "#14 10.16 Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n",
      "#14 10.16 Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.8/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.3.0)\n",
      "#14 10.16 Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (4.9)\n",
      "#14 10.25 Collecting scikit-learn>=0.21.3 (from seqeval->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 10.26   Downloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "#14 10.29 Collecting array-record (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 10.30   Downloading array_record-0.4.0-py38-none-any.whl.metadata (502 bytes)\n",
      "#14 10.32 Collecting click (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 10.32   Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "#14 10.35 Collecting etils>=0.9.0 (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 10.36   Downloading etils-1.3.0-py3-none-any.whl.metadata (5.5 kB)\n",
      "#14 10.39 Collecting promise (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 10.40   Downloading promise-2.3.tar.gz (19 kB)\n",
      "#14 10.41   Preparing metadata (setup.py): started\n",
      "#14 10.56   Preparing metadata (setup.py): finished with status 'done'\n",
      "#14 10.73 Collecting tensorflow-metadata (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 10.74   Downloading tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "#14 10.75 Collecting toml (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 10.76   Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "#14 10.77 Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (0.40.0)\n",
      "#14 10.82 Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 10.83   Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "#14 11.01 Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (5.3.1)\n",
      "#14 11.09 Collecting joblib>=1.1.1 (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 11.14   Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "#14 11.16 Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 11.17   Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "#14 11.19 Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.0)\n",
      "#14 11.19 Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (3.4.3)\n",
      "#14 11.20 Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (0.7.1)\n",
      "#14 11.21 Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (2.3.6)\n",
      "#14 11.31 INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
      "#14 11.31 Collecting tf-keras>=2.14.1 (from tensorflow-hub>=0.6.0->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "#14 11.32   Downloading tf_keras-2.15.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "#14 11.46 Collecting protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 (from apache-beam->object-detection==0.1)\n",
      "#14 11.46   Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (679 bytes)\n",
      "#14 11.49 Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.1)\n",
      "#14 11.60 Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (6.7.0)\n",
      "#14 11.61 Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (2.1.3)\n",
      "#14 11.62 Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (3.2.2)\n",
      "#14 11.65 Downloading Pillow-9.5.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "#14 11.68    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 91.1 MB/s eta 0:00:00\n",
      "#14 11.69 Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "#14 11.72 Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
      "#14 11.74 Downloading tf_models_official-2.13.2-py2.py3-none-any.whl (2.6 MB)\n",
      "#14 11.78    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.6/2.6 MB 65.3 MB/s eta 0:00:00\n",
      "#14 11.78 Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "#14 11.86    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.4/12.4 MB 160.2 MB/s eta 0:00:00\n",
      "#14 11.87 Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "#14 12.12    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.5/34.5 MB 140.0 MB/s eta 0:00:00\n",
      "#14 12.12 Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "#14 12.13 Downloading apache_beam-2.60.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.2 MB)\n",
      "#14 12.42    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.2/16.2 MB 58.7 MB/s eta 0:00:00\n",
      "#14 12.42 Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "#14 12.43 Downloading cython-3.1.4-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "#14 12.47    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 92.7 MB/s eta 0:00:00\n",
      "#14 12.47 Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
      "#14 12.48 Downloading matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "#14 12.61    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.2/9.2 MB 69.4 MB/s eta 0:00:00\n",
      "#14 12.62 Downloading pycocotools-2.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (439 kB)\n",
      "#14 12.63 Downloading tensorflow_io-0.34.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (28.8 MB)\n",
      "#14 13.54    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 28.8/28.8 MB 31.5 MB/s eta 0:00:00\n",
      "#14 13.56 Downloading tensorflow_io_gcs_filesystem-0.34.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "#14 13.60    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 65.7 MB/s eta 0:00:00\n",
      "#14 13.60 Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "#14 13.61 Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "#14 13.64 Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "#14 13.65 Downloading fastavro-1.9.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "#14 13.67    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 117.1 MB/s eta 0:00:00\n",
      "#14 13.68 Downloading fasteners-0.20-py3-none-any.whl (18 kB)\n",
      "#14 13.69 Downloading fonttools-4.57.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "#14 13.74    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 92.0 MB/s eta 0:00:00\n",
      "#14 13.75 Downloading google_api_python_client-2.184.0-py3-none-any.whl (14.3 MB)\n",
      "#14 13.89    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.3/14.3 MB 100.0 MB/s eta 0:00:00\n",
      "#14 13.90 Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "#14 13.90 Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "#14 13.91 Downloading jsonpickle-3.4.2-py3-none-any.whl (46 kB)\n",
      "#14 13.92 Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "#14 13.93 Downloading kaggle-1.7.4.5-py3-none-any.whl (181 kB)\n",
      "#14 13.94 Downloading kiwisolver-1.4.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "#14 13.95    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 111.5 MB/s eta 0:00:00\n",
      "#14 13.96 Downloading objsize-0.7.1-py3-none-any.whl (11 kB)\n",
      "#14 13.97 Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
      "#14 15.31    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 MB 51.2 MB/s eta 0:00:00\n",
      "#14 15.31 Downloading orjson-3.10.15-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "#14 15.32 Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "#14 15.33 Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n",
      "#14 15.34 Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "#14 15.34 Downloading pyarrow-16.1.0-cp38-cp38-manylinux_2_28_x86_64.whl (40.9 MB)\n",
      "#14 15.92    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 MB 70.5 MB/s eta 0:00:00\n",
      "#14 15.99 Downloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
      "#14 15.99 Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "#14 16.00 Downloading pymongo-4.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (930 kB)\n",
      "#14 16.01    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 930.2/930.2 kB 79.9 MB/s eta 0:00:00\n",
      "#14 16.02 Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "#14 16.03 Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "#14 16.04 Downloading PyYAML-6.0.3-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n",
      "#14 16.05    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 806.0/806.0 kB 69.8 MB/s eta 0:00:00\n",
      "#14 16.05 Downloading redis-5.3.1-py3-none-any.whl (272 kB)\n",
      "#14 16.06 Downloading regex-2024.11.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "#14 16.07    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 785.1/785.1 kB 72.0 MB/s eta 0:00:00\n",
      "#14 16.10 Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "#14 16.11 Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "#14 16.11 Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "#14 16.13 Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
      "#14 16.14 Downloading tensorflow_text-2.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "#14 16.79    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/6.5 MB 9.8 MB/s eta 0:00:00\n",
      "#14 16.79 Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "#14 16.80 Downloading zstandard-0.23.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "#14 16.87    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 79.1 MB/s eta 0:00:00\n",
      "#14 16.88 Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "#14 16.88 Downloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "#14 16.89 Downloading immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
      "#14 16.90 Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "#14 16.90 Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n",
      "#14 17.62    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 MB 76.1 MB/s eta 0:00:00\n",
      "#14 17.62 Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
      "#14 17.63 Downloading sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "#14 17.65    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 62.1 MB/s eta 0:00:00\n",
      "#14 17.66 Downloading tensorflow_datasets-4.9.2-py3-none-any.whl (5.4 MB)\n",
      "#14 17.72    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 87.2 MB/s eta 0:00:00\n",
      "#14 17.72 Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "#14 17.73 Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "#14 17.74 Downloading charset_normalizer-3.4.3-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (147 kB)\n",
      "#14 17.75 Downloading dm_tree-0.1.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "#14 17.76 Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "#14 17.77 Downloading etils-1.3.0-py3-none-any.whl (126 kB)\n",
      "#14 17.78 Downloading google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "#14 17.79 Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "#14 17.79 Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "#14 17.80 Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\n",
      "#14 17.80 Downloading PyJWT-2.9.0-py3-none-any.whl (22 kB)\n",
      "#14 17.81 Downloading referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "#14 17.82 Downloading rpds_py-0.20.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (360 kB)\n",
      "#14 17.83 Downloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
      "#14 17.94    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.1/11.1 MB 102.5 MB/s eta 0:00:00\n",
      "#14 17.95 Downloading tf_keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "#14 17.96    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 122.9 MB/s eta 0:00:00\n",
      "#14 17.97 Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "#14 17.98 Downloading array_record-0.4.0-py38-none-any.whl (3.0 MB)\n",
      "#14 18.03    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 64.5 MB/s eta 0:00:00\n",
      "#14 18.03 Downloading bleach-6.1.0-py3-none-any.whl (162 kB)\n",
      "#14 18.04 Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "#14 18.05 Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "#14 18.06 Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "#14 18.06 Downloading tensorflow_metadata-1.14.0-py3-none-any.whl (28 kB)\n",
      "#14 18.07 Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "#14 18.09    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 60.4 MB/s eta 0:00:00\n",
      "#14 18.09 Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "#14 18.10 Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "#14 18.10 Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "#14 18.12 Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "#14 18.13 Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "#14 18.50 Building wheels for collected packages: object-detection, avro-python3, crcmod, dill, hdfs, seqeval, docopt, promise\n",
      "#14 18.50   Building wheel for object-detection (setup.py): started\n",
      "#14 19.13   Building wheel for object-detection (setup.py): finished with status 'done'\n",
      "#14 19.13   Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1466904 sha256=fa5976b96ba8b15f9437ee6bb559ddbb924042c095e46951769e1e4534992e19\n",
      "#14 19.13   Stored in directory: /tmp/pip-ephem-wheel-cache-yohx4y0t/wheels/28/d2/ce/f2754826bc8f50adf45d76a4c3cffa1a58dd936429295e0ddd\n",
      "#14 19.14   Building wheel for avro-python3 (setup.py): started\n",
      "#14 19.44   Building wheel for avro-python3 (setup.py): finished with status 'done'\n",
      "#14 19.44   Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=43994 sha256=4b4613a546c83d8d7af8690b969d3a0cbf07a523e05f53f31eb8f1dd117c2667\n",
      "#14 19.44   Stored in directory: /root/.cache/pip/wheels/bb/73/e9/d273421f5723c4bf544dcf9eb097bda94421ef8d3252699f0a\n",
      "#14 19.44   Building wheel for crcmod (setup.py): started\n",
      "#14 19.84   Building wheel for crcmod (setup.py): finished with status 'done'\n",
      "#14 19.85   Created wheel for crcmod: filename=crcmod-1.7-cp38-cp38-linux_x86_64.whl size=36020 sha256=0c125cfe66fe543ed04a1639907227982fdab154653de53e74ff5bbe4c1f3b1f\n",
      "#14 19.85   Stored in directory: /root/.cache/pip/wheels/ca/5a/02/f3acf982a026f3319fb3e798a8dca2d48fafee7761788562e9\n",
      "#14 19.85   Building wheel for dill (setup.py): started\n",
      "#14 20.07   Building wheel for dill (setup.py): finished with status 'done'\n",
      "#14 20.07   Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78545 sha256=ba0522aa6912deff5c441c26a33e25301b8203f75175c5289d388dc5cdf4a5ca\n",
      "#14 20.07   Stored in directory: /root/.cache/pip/wheels/07/35/78/e9004fa30578734db7f10e7a211605f3f0778d2bdde38a239d\n",
      "#14 20.08   Building wheel for hdfs (setup.py): started\n",
      "#14 20.29   Building wheel for hdfs (setup.py): finished with status 'done'\n",
      "#14 20.29   Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34328 sha256=9acdc11aa5625cd20d3e37b9079c2acd1c3bbdb4b9f9b5460529341d30cf5f20\n",
      "#14 20.29   Stored in directory: /root/.cache/pip/wheels/68/dd/29/c1a590238f9ebbe4f7ee9b3583f5185d0b9577e23f05c990eb\n",
      "#14 20.29   Building wheel for seqeval (setup.py): started\n",
      "#14 20.61   Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "#14 20.61   Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16165 sha256=9ab2dda4b03cc8b67568e00b11fb85218ed6470bee2c580bbce43b885f1dd29d\n",
      "#14 20.61   Stored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
      "#14 20.62   Building wheel for docopt (setup.py): started\n",
      "#14 20.82   Building wheel for docopt (setup.py): finished with status 'done'\n",
      "#14 20.82   Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=db30e33c707db7298e3b630569cb8072cf0c7372a35d9f16d7c00a55222a14dd\n",
      "#14 20.82   Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
      "#14 20.82   Building wheel for promise (setup.py): started\n",
      "#14 21.03   Building wheel for promise (setup.py): finished with status 'done'\n",
      "#14 21.03   Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21484 sha256=55321025151cddea5af354534e20e55940612f9a18c650d1787f5e7e2771d397\n",
      "#14 21.04   Stored in directory: /root/.cache/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "#14 21.04 Successfully built object-detection avro-python3 crcmod dill hdfs seqeval docopt promise\n",
      "#14 21.44 Installing collected packages: text-unidecode, sentencepiece, pytz, py-cpuinfo, gin-config, docopt, dm-tree, crcmod, zstandard, uritemplate, tzdata, tqdm, toml, threadpoolctl, tf-slim, tf-keras, tensorflow-model-optimization, tensorflow-io-gcs-filesystem, tabulate, scipy, rpds-py, regex, pyyaml, python-slugify, python-dateutil, pyparsing, PyJWT, pyarrow-hotfix, pyarrow, psutil, protobuf, promise, portalocker, pkgutil-resolve-name, pillow, orjson, opencv-python-headless, opencv-python, objsize, kiwisolver, jsonpickle, joblib, importlib-resources, immutabledict, fonttools, fasteners, fastavro, etils, dnspython, dill, Cython, cycler, contourpy, contextlib2, colorama, cloudpickle, click, charset-normalizer, bleach, avro-python3, attrs, async-timeout, tensorflow_io, tensorflow-hub, scikit-learn, sacrebleu, requests, referencing, redis, pymongo, pydot, proto-plus, pandas, matplotlib, httplib2, googleapis-common-protos, tensorflow-metadata, seqeval, pycocotools, oauth2client, lvis, kaggle, jsonschema-specifications, hdfs, google-auth-httplib2, google-api-core, jsonschema, google-api-python-client, array-record, tensorflow-datasets, apache-beam, tensorflow-text, tf-models-official, object-detection\n",
      "#14 22.95   Attempting uninstall: tensorflow-io-gcs-filesystem\n",
      "#14 22.95     Found existing installation: tensorflow-io-gcs-filesystem 0.32.0\n",
      "#14 22.95     Uninstalling tensorflow-io-gcs-filesystem-0.32.0:\n",
      "#14 22.96       Successfully uninstalled tensorflow-io-gcs-filesystem-0.32.0\n",
      "#14 26.80   Attempting uninstall: protobuf\n",
      "#14 26.81     Found existing installation: protobuf 3.20.1\n",
      "#14 26.81     Uninstalling protobuf-3.20.1:\n",
      "#14 27.03       Successfully uninstalled protobuf-3.20.1\n",
      "#14 27.15   Attempting uninstall: pillow\n",
      "#14 27.15     Found existing installation: Pillow 7.0.0\n",
      "#14 27.15     Uninstalling Pillow-7.0.0:\n",
      "#14 27.19       Successfully uninstalled Pillow-7.0.0\n",
      "#14 33.00   Attempting uninstall: requests\n",
      "#14 33.00     Found existing installation: requests 2.22.0\n",
      "#14 33.00     Uninstalling requests-2.22.0:\n",
      "#14 33.01       Successfully uninstalled requests-2.22.0\n",
      "#14 43.68 Successfully installed Cython-3.1.4 PyJWT-2.9.0 apache-beam-2.60.0 array-record-0.4.0 async-timeout-5.0.1 attrs-25.3.0 avro-python3-1.10.2 bleach-6.1.0 charset-normalizer-3.4.3 click-8.1.8 cloudpickle-2.2.1 colorama-0.4.6 contextlib2-21.6.0 contourpy-1.1.1 crcmod-1.7 cycler-0.12.1 dill-0.3.1.1 dm-tree-0.1.8 dnspython-2.6.1 docopt-0.6.2 etils-1.3.0 fastavro-1.9.7 fasteners-0.20 fonttools-4.57.0 gin-config-0.5.0 google-api-core-2.25.1 google-api-python-client-2.184.0 google-auth-httplib2-0.2.0 googleapis-common-protos-1.70.0 hdfs-2.7.3 httplib2-0.22.0 immutabledict-4.2.1 importlib-resources-6.4.5 joblib-1.4.2 jsonpickle-3.4.2 jsonschema-4.23.0 jsonschema-specifications-2023.12.1 kaggle-1.7.4.5 kiwisolver-1.4.7 lvis-0.5.3 matplotlib-3.7.5 oauth2client-4.1.3 object-detection-0.1 objsize-0.7.1 opencv-python-4.12.0.88 opencv-python-headless-4.12.0.88 orjson-3.10.15 pandas-2.0.3 pillow-9.5.0 pkgutil-resolve-name-1.3.10 portalocker-3.0.0 promise-2.3 proto-plus-1.26.1 protobuf-3.20.3 psutil-7.1.0 py-cpuinfo-9.0.0 pyarrow-16.1.0 pyarrow-hotfix-0.7 pycocotools-2.0.7 pydot-1.4.2 pymongo-4.10.1 pyparsing-2.4.7 python-dateutil-2.9.0.post0 python-slugify-8.0.4 pytz-2025.2 pyyaml-6.0.3 redis-5.3.1 referencing-0.35.1 regex-2024.11.6 requests-2.32.4 rpds-py-0.20.1 sacrebleu-2.2.0 scikit-learn-1.3.2 scipy-1.10.1 sentencepiece-0.2.0 seqeval-1.2.2 tabulate-0.9.0 tensorflow-datasets-4.9.2 tensorflow-hub-0.16.1 tensorflow-io-gcs-filesystem-0.34.0 tensorflow-metadata-1.14.0 tensorflow-model-optimization-0.8.0 tensorflow-text-2.13.0 tensorflow_io-0.34.0 text-unidecode-1.3 tf-keras-2.15.0 tf-models-official-2.13.2 tf-slim-1.1.0 threadpoolctl-3.5.0 toml-0.10.2 tqdm-4.67.1 tzdata-2025.2 uritemplate-4.1.1 zstandard-0.23.0\n",
      "#14 43.68 WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "#14 DONE 51.9s\n",
      "\n",
      "#15 [11/12] RUN pip3 install sagemaker-training\n",
      "#15 0.556 Collecting sagemaker-training\n",
      "#15 0.581   Downloading sagemaker_training-5.1.1.tar.gz (59 kB)\n",
      "#15 0.601   Preparing metadata (setup.py): started\n",
      "#15 0.845   Preparing metadata (setup.py): finished with status 'done'\n",
      "#15 0.851 Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sagemaker-training) (1.24.3)\n",
      "#15 1.179 Collecting boto3 (from sagemaker-training)\n",
      "#15 1.186   Downloading boto3-1.37.38-py3-none-any.whl.metadata (6.7 kB)\n",
      "#15 1.193 Requirement already satisfied: six in /usr/lib/python3/dist-packages (from sagemaker-training) (1.14.0)\n",
      "#15 1.194 Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (from sagemaker-training) (25.0.1)\n",
      "#15 1.205 Collecting retrying>=1.3.3 (from sagemaker-training)\n",
      "#15 1.212   Downloading retrying-1.4.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "#15 1.322 Collecting gevent (from sagemaker-training)\n",
      "#15 1.329   Downloading gevent-24.2.1-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "#15 1.363 Collecting inotify_simple==1.2.1 (from sagemaker-training)\n",
      "#15 1.369   Downloading inotify_simple-1.2.1.tar.gz (7.9 kB)\n",
      "#15 1.377   Preparing metadata (setup.py): started\n",
      "#15 1.541   Preparing metadata (setup.py): finished with status 'done'\n",
      "#15 1.543 Requirement already satisfied: werkzeug>=0.15.5 in /usr/local/lib/python3.8/dist-packages (from sagemaker-training) (2.3.6)\n",
      "#15 1.575 Collecting paramiko>=2.4.2 (from sagemaker-training)\n",
      "#15 1.605   Downloading paramiko-3.5.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "#15 1.613 Requirement already satisfied: psutil>=5.6.7 in /usr/local/lib/python3.8/dist-packages (from sagemaker-training) (7.1.0)\n",
      "#15 1.786 Collecting protobuf>=5.28.1 (from sagemaker-training)\n",
      "#15 1.789   Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "#15 1.795 Requirement already satisfied: scipy>=1.2.2 in /usr/local/lib/python3.8/dist-packages (from sagemaker-training) (1.10.1)\n",
      "#15 2.297 Collecting botocore>=1.31.57 (from sagemaker-training)\n",
      "#15 2.301   Downloading botocore-1.37.38-py3-none-any.whl.metadata (5.7 kB)\n",
      "#15 2.435 Collecting jmespath<2.0.0,>=0.7.1 (from boto3->sagemaker-training)\n",
      "#15 2.439   Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "#15 2.463 Collecting s3transfer<0.12.0,>=0.11.0 (from boto3->sagemaker-training)\n",
      "#15 2.468   Downloading s3transfer-0.11.5-py3-none-any.whl.metadata (1.7 kB)\n",
      "#15 2.476 Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore>=1.31.57->sagemaker-training) (2.9.0.post0)\n",
      "#15 2.476 Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/lib/python3/dist-packages (from botocore>=1.31.57->sagemaker-training) (1.25.8)\n",
      "#15 2.518 Collecting bcrypt>=3.2 (from paramiko>=2.4.2->sagemaker-training)\n",
      "#15 2.525   Downloading bcrypt-5.0.0-cp38-abi3-manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "#15 2.713 Collecting cryptography>=3.3 (from paramiko>=2.4.2->sagemaker-training)\n",
      "#15 2.717   Downloading cryptography-46.0.2-cp38-abi3-manylinux_2_28_x86_64.whl.metadata (5.7 kB)\n",
      "#15 2.749 Collecting pynacl>=1.5 (from paramiko>=2.4.2->sagemaker-training)\n",
      "#15 2.753   Downloading pynacl-1.6.0-cp38-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (9.4 kB)\n",
      "#15 2.775 Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=0.15.5->sagemaker-training) (2.1.3)\n",
      "#15 2.788 Collecting zope.event (from gevent->sagemaker-training)\n",
      "#15 2.795   Downloading zope.event-5.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "#15 2.911 Collecting zope.interface (from gevent->sagemaker-training)\n",
      "#15 2.946   Downloading zope.interface-7.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "#15 3.129 Collecting greenlet>=2.0.0 (from gevent->sagemaker-training)\n",
      "#15 3.133   Downloading greenlet-3.1.1-cp38-cp38-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "#15 3.143 Requirement already satisfied: cffi>=1.14 in /usr/lib/python3/dist-packages (from cryptography>=3.3->paramiko>=2.4.2->sagemaker-training) (1.14.0)\n",
      "#15 3.160 Collecting typing-extensions>=4.13.2 (from cryptography>=3.3->paramiko>=2.4.2->sagemaker-training)\n",
      "#15 3.167   Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "#15 3.294 Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from zope.event->gevent->sagemaker-training) (68.0.0)\n",
      "#15 3.312 Downloading boto3-1.37.38-py3-none-any.whl (139 kB)\n",
      "#15 3.324 Downloading botocore-1.37.38-py3-none-any.whl (13.5 MB)\n",
      "#15 3.400    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.5/13.5 MB 186.0 MB/s eta 0:00:00\n",
      "#15 3.404 Downloading paramiko-3.5.1-py3-none-any.whl (227 kB)\n",
      "#15 3.412 Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "#15 3.425 Downloading retrying-1.4.2-py3-none-any.whl (10 kB)\n",
      "#15 3.435 Downloading gevent-24.2.1-cp38-cp38-manylinux_2_28_x86_64.whl (6.7 MB)\n",
      "#15 3.510    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.7/6.7 MB 89.7 MB/s eta 0:00:00\n",
      "#15 3.523 Downloading bcrypt-5.0.0-cp38-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
      "#15 3.537 Downloading cryptography-46.0.2-cp38-abi3-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "#15 3.594    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 79.9 MB/s eta 0:00:00\n",
      "#15 3.604 Downloading greenlet-3.1.1-cp38-cp38-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (605 kB)\n",
      "#15 3.629    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 606.0/606.0 kB 14.9 MB/s eta 0:00:00\n",
      "#15 3.642 Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "#15 3.649 Downloading pynacl-1.6.0-cp38-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "#15 3.667    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 81.8 MB/s eta 0:00:00\n",
      "#15 3.671 Downloading s3transfer-0.11.5-py3-none-any.whl (84 kB)\n",
      "#15 3.679 Downloading zope.event-5.0-py3-none-any.whl (6.8 kB)\n",
      "#15 3.699 Downloading zope.interface-7.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (257 kB)\n",
      "#15 3.725 Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "#15 3.774 Building wheels for collected packages: sagemaker-training, inotify_simple\n",
      "#15 3.774   Building wheel for sagemaker-training (setup.py): started\n",
      "#15 4.426   Building wheel for sagemaker-training (setup.py): finished with status 'done'\n",
      "#15 4.427   Created wheel for sagemaker-training: filename=sagemaker_training-5.1.1-cp38-cp38-linux_x86_64.whl size=92755 sha256=a0c09efc80aa7b7e9fb2e742dcee764e1f7750205635c9b169edb55006115944\n",
      "#15 4.427   Stored in directory: /root/.cache/pip/wheels/77/ad/06/8188c6d965cddc058810e64fe7ce6449a0e7da648186f85003\n",
      "#15 4.430   Building wheel for inotify_simple (setup.py): started\n",
      "#15 4.711   Building wheel for inotify_simple (setup.py): finished with status 'done'\n",
      "#15 4.711   Created wheel for inotify_simple: filename=inotify_simple-1.2.1-py3-none-any.whl size=8201 sha256=b4f48cf34a88273b3dd5f9a07b8220e5fb2e2bea162bf91e49af9d905daf1dab\n",
      "#15 4.711   Stored in directory: /root/.cache/pip/wheels/8b/2d/c2/46bac8503a2469925f6f463615984b3d1fe472e729363a28d3\n",
      "#15 4.713 Successfully built sagemaker-training inotify_simple\n",
      "#15 5.062 Installing collected packages: inotify_simple, zope.interface, zope.event, typing-extensions, retrying, pynacl, protobuf, jmespath, greenlet, bcrypt, gevent, cryptography, botocore, s3transfer, paramiko, boto3, sagemaker-training\n",
      "#15 5.193   Attempting uninstall: typing-extensions\n",
      "#15 5.194     Found existing installation: typing_extensions 4.5.0\n",
      "#15 5.196     Uninstalling typing_extensions-4.5.0:\n",
      "#15 5.516       Successfully uninstalled typing_extensions-4.5.0\n",
      "#15 5.601   Attempting uninstall: protobuf\n",
      "#15 5.602     Found existing installation: protobuf 3.20.3\n",
      "#15 5.608     Uninstalling protobuf-3.20.3:\n",
      "#15 5.906       Successfully uninstalled protobuf-3.20.3\n",
      "#15 7.341 ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "#15 7.341 apache-beam 2.60.0 requires protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3, but you have protobuf 5.29.5 which is incompatible.\n",
      "#15 7.341 tensorflow 2.13.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.5 which is incompatible.\n",
      "#15 7.341 tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.13.2 which is incompatible.\n",
      "#15 7.341 tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 5.29.5 which is incompatible.\n",
      "#15 7.341 Successfully installed bcrypt-5.0.0 boto3-1.37.38 botocore-1.37.38 cryptography-46.0.2 gevent-24.2.1 greenlet-3.1.1 inotify_simple-1.2.1 jmespath-1.0.1 paramiko-3.5.1 protobuf-5.29.5 pynacl-1.6.0 retrying-1.4.2 s3transfer-0.11.5 sagemaker-training-5.1.1 typing-extensions-4.13.2 zope.event-5.0 zope.interface-7.2\n",
      "#15 7.341 WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "#15 DONE 7.7s\n",
      "\n",
      "#16 [12/12] RUN pip install \"opencv-python-headless<4.3\"\n",
      "#16 0.626 Collecting opencv-python-headless<4.3\n",
      "#16 0.637   Downloading opencv_python_headless-4.2.0.34-cp38-cp38-manylinux1_x86_64.whl.metadata (11 kB)\n",
      "#16 0.643 Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from opencv-python-headless<4.3) (1.24.3)\n",
      "#16 0.655 Downloading opencv_python_headless-4.2.0.34-cp38-cp38-manylinux1_x86_64.whl (21.6 MB)\n",
      "#16 1.107    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.6/21.6 MB 48.1 MB/s eta 0:00:00\n",
      "#16 1.451 Installing collected packages: opencv-python-headless\n",
      "#16 1.451   Attempting uninstall: opencv-python-headless\n",
      "#16 1.452     Found existing installation: opencv-python-headless 4.12.0.88\n",
      "#16 1.459     Uninstalling opencv-python-headless-4.12.0.88:\n",
      "#16 1.667       Successfully uninstalled opencv-python-headless-4.12.0.88\n",
      "#16 2.034 Successfully installed opencv-python-headless-4.2.0.34\n",
      "#16 2.035 WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "#16 DONE 2.2s\n",
      "\n",
      "#17 exporting to image\n",
      "#17 exporting layers\n",
      "#17 exporting layers 4.6s done\n",
      "#17 writing image sha256:d4d4def1d9b1be3b94f610034d79c3abbd28ace4f4d97005027595e06737d317 done\n",
      "#17 naming to docker.io/library/tf2-object-detection done\n",
      "#17 DONE 4.6s\n",
      "Pushing image to ECR 738138308218.dkr.ecr.us-east-1.amazonaws.com/tf2-object-detection:20251002013034\n",
      "The push refers to repository [738138308218.dkr.ecr.us-east-1.amazonaws.com/tf2-object-detection]\n",
      "9870c996a9f9: Preparing\n",
      "e375545a29db: Preparing\n",
      "dbfd10991a41: Preparing\n",
      "eb40683b76b3: Preparing\n",
      "948696ba83ce: Preparing\n",
      "d99abddbab57: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "983e779b2d63: Preparing\n",
      "2cb6df91de51: Preparing\n",
      "47d15b2ac51f: Preparing\n",
      "a47495f79503: Preparing\n",
      "a073a40e4dcd: Preparing\n",
      "2ec1b5695a98: Preparing\n",
      "0b37f0d116f4: Preparing\n",
      "da5d2813a979: Preparing\n",
      "0eaa6e868aa5: Preparing\n",
      "1ca9136c7d36: Preparing\n",
      "dca2891e0e76: Preparing\n",
      "eaa2ed848ac5: Preparing\n",
      "ff3418b47754: Preparing\n",
      "d3dd91e05b94: Preparing\n",
      "d49fe103257c: Preparing\n",
      "e67ab25399cb: Preparing\n",
      "9a09b667a965: Preparing\n",
      "93b76ad9c95e: Preparing\n",
      "a2fdb4e1ecd1: Preparing\n",
      "0ceb5c845fcf: Preparing\n",
      "6426a7216f78: Preparing\n",
      "ec66d8cea54a: Preparing\n",
      "1ca9136c7d36: Waiting\n",
      "dca2891e0e76: Waiting\n",
      "eaa2ed848ac5: Waiting\n",
      "ff3418b47754: Waiting\n",
      "d3dd91e05b94: Waiting\n",
      "d99abddbab57: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "983e779b2d63: Waiting\n",
      "2cb6df91de51: Waiting\n",
      "47d15b2ac51f: Waiting\n",
      "a47495f79503: Waiting\n",
      "a073a40e4dcd: Waiting\n",
      "2ec1b5695a98: Waiting\n",
      "0b37f0d116f4: Waiting\n",
      "da5d2813a979: Waiting\n",
      "0eaa6e868aa5: Waiting\n",
      "d49fe103257c: Waiting\n",
      "e67ab25399cb: Waiting\n",
      "9a09b667a965: Waiting\n",
      "93b76ad9c95e: Waiting\n",
      "a2fdb4e1ecd1: Waiting\n",
      "0ceb5c845fcf: Waiting\n",
      "6426a7216f78: Waiting\n",
      "ec66d8cea54a: Waiting\n",
      "948696ba83ce: Pushed\n",
      "d99abddbab57: Pushed\n",
      "eb40683b76b3: Pushed\n",
      "983e779b2d63: Pushed\n",
      "5f70bf18a086: Pushed\n",
      "47d15b2ac51f: Pushed\n",
      "2cb6df91de51: Pushed\n",
      "a073a40e4dcd: Layer already exists\n",
      "9870c996a9f9: Pushed\n",
      "2ec1b5695a98: Layer already exists\n",
      "0b37f0d116f4: Layer already exists\n",
      "da5d2813a979: Layer already exists\n",
      "0eaa6e868aa5: Layer already exists\n",
      "1ca9136c7d36: Layer already exists\n",
      "dca2891e0e76: Layer already exists\n",
      "eaa2ed848ac5: Layer already exists\n",
      "ff3418b47754: Layer already exists\n",
      "d3dd91e05b94: Layer already exists\n",
      "d49fe103257c: Layer already exists\n",
      "e67ab25399cb: Layer already exists\n",
      "9a09b667a965: Layer already exists\n",
      "93b76ad9c95e: Layer already exists\n",
      "0ceb5c845fcf: Layer already exists\n",
      "a2fdb4e1ecd1: Layer already exists\n",
      "6426a7216f78: Layer already exists\n",
      "e375545a29db: Pushed\n",
      "ec66d8cea54a: Pushed\n",
      "a47495f79503: Pushed\n",
      "dbfd10991a41: Pushed\n",
      "20251002013034: digest: sha256:80bd0f100ba88593a0664c363cf392d8f2c81e541b485fe922f63a31df8b5dde size: 6398\n",
      "Saving ECR image URI into ecr_image_fullname.txt\n"
     ]
    }
   ],
   "source": [
    "# build and push the docker image. This code can be commented out after being run once.\n",
    "# This will take around 10 mins.\n",
    "image_name = 'tf2-object-detection'\n",
    "!sh ./docker/build_and_push.sh $image_name 2>&1 | tail -n 500"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e62b3562",
   "metadata": {},
   "source": [
    "To verify that the image was correctly pushed to the [Elastic Container Registry](https://aws.amazon.com/ecr/), you can look at it in the AWS webapp. For example, below you can see that three different images have been pushed to ECR. You should only see one, called `tf2-object-detection`.\n",
    "![ECR Example](../data/example_ecr.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0310b6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738138308218.dkr.ecr.us-east-1.amazonaws.com/tf2-object-detection:20251002013034\n"
     ]
    }
   ],
   "source": [
    "# display the container name\n",
    "with open (os.path.join('docker', 'ecr_image_fullname.txt'), 'r') as f:\n",
    "    container = f.readlines()[0][:-1]\n",
    "\n",
    "print(container)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13b2a754",
   "metadata": {},
   "source": [
    "## Pre-trained model from model zoo\n",
    "\n",
    "As often, we are not training from scratch and we will be using a pretrained model from the TF Object Detection model zoo. You can find pretrained checkpoints [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). Because your time is limited for this project, we recommend to only experiment with the following models:\n",
    "* SSD MobileNet V2 FPNLite 640x640\t\n",
    "* SSD ResNet50 V1 FPN 640x640 (RetinaNet50)\t\n",
    "* Faster R-CNN ResNet50 V1 640x640\t\n",
    "* EfficientDet D1 640x640\t\n",
    "* Faster R-CNN ResNet152 V1 640x640\t\n",
    "\n",
    "In the code below, the EfficientDet D1 model is downloaded and extracted. This code should be adjusted if you were to experiment with other architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c4b1d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘source_dir/checkpoint’: File exists\n",
      "--2025-10-02 01:34:41--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz\n",
      "Resolving download.tensorflow.org (download.tensorflow.org)... 172.253.62.207, 142.251.163.207, 142.251.167.207, ...\n",
      "Connecting to download.tensorflow.org (download.tensorflow.org)|172.253.62.207|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20518283 (20M) [application/x-tar]\n",
      "Saving to: ‘/tmp/efficientdet.tar.gz’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0% 13.6M 1s\n",
      "    50K .......... .......... .......... .......... ..........  0% 24.5M 1s\n",
      "   100K .......... .......... .......... .......... ..........  0% 26.2M 1s\n",
      "   150K .......... .......... .......... .......... ..........  0% 26.4M 1s\n",
      "   200K .......... .......... .......... .......... ..........  1% 27.1M 1s\n",
      "   250K .......... .......... .......... .......... ..........  1% 25.5M 1s\n",
      "   300K .......... .......... .......... .......... ..........  1% 25.9M 1s\n",
      "   350K .......... .......... .......... .......... ..........  1% 93.4M 1s\n",
      "   400K .......... .......... .......... .......... ..........  2% 20.9M 1s\n",
      "   450K .......... .......... .......... .......... ..........  2% 31.7M 1s\n",
      "   500K .......... .......... .......... .......... ..........  2% 47.4M 1s\n",
      "   550K .......... .......... .......... .......... ..........  2% 40.0M 1s\n",
      "   600K .......... .......... .......... .......... ..........  3% 45.4M 1s\n",
      "   650K .......... .......... .......... .......... ..........  3% 39.5M 1s\n",
      "   700K .......... .......... .......... .......... ..........  3% 46.9M 1s\n",
      "   750K .......... .......... .......... .......... ..........  3% 41.2M 1s\n",
      "   800K .......... .......... .......... .......... ..........  4% 45.5M 1s\n",
      "   850K .......... .......... .......... .......... ..........  4% 41.2M 1s\n",
      "   900K .......... .......... .......... .......... ..........  4% 39.0M 1s\n",
      "   950K .......... .......... .......... .......... ..........  4% 44.4M 1s\n",
      "  1000K .......... .......... .......... .......... ..........  5% 43.1M 1s\n",
      "  1050K .......... .......... .......... .......... ..........  5% 42.1M 1s\n",
      "  1100K .......... .......... .......... .......... ..........  5% 38.5M 1s\n",
      "  1150K .......... .......... .......... .......... ..........  5% 43.8M 1s\n",
      "  1200K .......... .......... .......... .......... ..........  6% 43.0M 1s\n",
      "  1250K .......... .......... .......... .......... ..........  6% 22.5M 1s\n",
      "  1300K .......... .......... .......... .......... ..........  6% 43.3M 1s\n",
      "  1350K .......... .......... .......... .......... ..........  6% 41.4M 1s\n",
      "  1400K .......... .......... .......... .......... ..........  7% 41.8M 1s\n",
      "  1450K .......... .......... .......... .......... ..........  7% 41.5M 1s\n",
      "  1500K .......... .......... .......... .......... ..........  7% 42.3M 1s\n",
      "  1550K .......... .......... .......... .......... ..........  7% 39.8M 1s\n",
      "  1600K .......... .......... .......... .......... ..........  8% 16.5M 1s\n",
      "  1650K .......... .......... .......... .......... ..........  8%  317M 1s\n",
      "  1700K .......... .......... .......... .......... ..........  8%  140M 1s\n",
      "  1750K .......... .......... .......... .......... ..........  8% 42.2M 1s\n",
      "  1800K .......... .......... .......... .......... ..........  9% 40.9M 1s\n",
      "  1850K .......... .......... .......... .......... ..........  9% 41.6M 1s\n",
      "  1900K .......... .......... .......... .......... ..........  9% 42.6M 0s\n",
      "  1950K .......... .......... .......... .......... ..........  9% 43.5M 0s\n",
      "  2000K .......... .......... .......... .......... .......... 10% 40.7M 0s\n",
      "  2050K .......... .......... .......... .......... .......... 10% 41.9M 0s\n",
      "  2100K .......... .......... .......... .......... .......... 10% 41.4M 0s\n",
      "  2150K .......... .......... .......... .......... .......... 10% 42.6M 0s\n",
      "  2200K .......... .......... .......... .......... .......... 11% 41.0M 0s\n",
      "  2250K .......... .......... .......... .......... .......... 11% 43.5M 0s\n",
      "  2300K .......... .......... .......... .......... .......... 11% 40.1M 0s\n",
      "  2350K .......... .......... .......... .......... .......... 11% 43.2M 0s\n",
      "  2400K .......... .......... .......... .......... .......... 12% 41.7M 0s\n",
      "  2450K .......... .......... .......... .......... .......... 12% 42.4M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 12% 42.2M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 12% 38.7M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 13% 40.9M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 13% 45.6M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 13% 41.2M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 13% 43.8M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 14% 40.2M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 14% 42.2M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 14% 44.0M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 14% 39.9M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 15% 42.1M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 15% 38.0M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 15% 45.5M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 15% 42.2M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 16% 41.1M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 16% 43.6M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 16% 43.6M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 16% 39.9M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 17% 41.4M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 17% 40.9M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 17% 41.0M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 17% 31.8M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 18% 55.9M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 18% 43.0M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 18% 41.8M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 18% 45.8M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 19% 41.2M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 19% 42.2M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 19% 42.5M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 19% 38.6M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 20% 15.9M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 20%  245M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 20%  101M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 20% 58.4M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 21% 34.6M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 21% 40.5M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 21% 54.4M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 21% 34.6M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 22% 57.7M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 22% 32.5M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 22% 57.9M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 22% 48.3M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 23% 31.0M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 23% 65.6M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 23% 41.8M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 23% 50.8M 0s\n",
      "  4800K .......... .......... .......... .......... .......... 24% 40.4M 0s\n",
      "  4850K .......... .......... .......... .......... .......... 24% 45.2M 0s\n",
      "  4900K .......... .......... .......... .......... .......... 24% 35.0M 0s\n",
      "  4950K .......... .......... .......... .......... .......... 24% 52.6M 0s\n",
      "  5000K .......... .......... .......... .......... .......... 25%  201M 0s\n",
      "  5050K .......... .......... .......... .......... .......... 25% 48.9M 0s\n",
      "  5100K .......... .......... .......... .......... .......... 25% 37.1M 0s\n",
      "  5150K .......... .......... .......... .......... .......... 25% 55.2M 0s\n",
      "  5200K .......... .......... .......... .......... .......... 26% 48.0M 0s\n",
      "  5250K .......... .......... .......... .......... .......... 26% 48.6M 0s\n",
      "  5300K .......... .......... .......... .......... .......... 26%  186M 0s\n",
      "  5350K .......... .......... .......... .......... .......... 26% 35.5M 0s\n",
      "  5400K .......... .......... .......... .......... .......... 27% 57.8M 0s\n",
      "  5450K .......... .......... .......... .......... .......... 27% 51.9M 0s\n",
      "  5500K .......... .......... .......... .......... .......... 27% 42.9M 0s\n",
      "  5550K .......... .......... .......... .......... .......... 27% 52.2M 0s\n",
      "  5600K .......... .......... .......... .......... .......... 28%  163M 0s\n",
      "  5650K .......... .......... .......... .......... .......... 28% 46.4M 0s\n",
      "  5700K .......... .......... .......... .......... .......... 28% 40.4M 0s\n",
      "  5750K .......... .......... .......... .......... .......... 28% 51.7M 0s\n",
      "  5800K .......... .......... .......... .......... .......... 29% 32.2M 0s\n",
      "  5850K .......... .......... .......... .......... .......... 29%  217M 0s\n",
      "  5900K .......... .......... .......... .......... .......... 29% 96.8M 0s\n",
      "  5950K .......... .......... .......... .......... .......... 29% 35.9M 0s\n",
      "  6000K .......... .......... .......... .......... .......... 30% 48.0M 0s\n",
      "  6050K .......... .......... .......... .......... .......... 30% 69.6M 0s\n",
      "  6100K .......... .......... .......... .......... .......... 30% 65.9M 0s\n",
      "  6150K .......... .......... .......... .......... .......... 30% 77.8M 0s\n",
      "  6200K .......... .......... .......... .......... .......... 31% 46.7M 0s\n",
      "  6250K .......... .......... .......... .......... .......... 31% 46.0M 0s\n",
      "  6300K .......... .......... .......... .......... .......... 31% 49.8M 0s\n",
      "  6350K .......... .......... .......... .......... .......... 31%  186M 0s\n",
      "  6400K .......... .......... .......... .......... .......... 32% 50.1M 0s\n",
      "  6450K .......... .......... .......... .......... .......... 32% 48.4M 0s\n",
      "  6500K .......... .......... .......... .......... .......... 32% 46.4M 0s\n",
      "  6550K .......... .......... .......... .......... .......... 32% 51.7M 0s\n",
      "  6600K .......... .......... .......... .......... .......... 33%  226M 0s\n",
      "  6650K .......... .......... .......... .......... .......... 33% 49.5M 0s\n",
      "  6700K .......... .......... .......... .......... .......... 33% 52.9M 0s\n",
      "  6750K .......... .......... .......... .......... .......... 33% 52.5M 0s\n",
      "  6800K .......... .......... .......... .......... .......... 34% 60.9M 0s\n",
      "  6850K .......... .......... .......... .......... .......... 34%  173M 0s\n",
      "  6900K .......... .......... .......... .......... .......... 34% 43.5M 0s\n",
      "  6950K .......... .......... .......... .......... .......... 34% 50.0M 0s\n",
      "  7000K .......... .......... .......... .......... .......... 35% 47.9M 0s\n",
      "  7050K .......... .......... .......... .......... .......... 35% 57.6M 0s\n",
      "  7100K .......... .......... .......... .......... .......... 35%  194M 0s\n",
      "  7150K .......... .......... .......... .......... .......... 35% 50.4M 0s\n",
      "  7200K .......... .......... .......... .......... .......... 36% 55.7M 0s\n",
      "  7250K .......... .......... .......... .......... .......... 36% 57.3M 0s\n",
      "  7300K .......... .......... .......... .......... .......... 36% 54.5M 0s\n",
      "  7350K .......... .......... .......... .......... .......... 36%  231M 0s\n",
      "  7400K .......... .......... .......... .......... .......... 37% 49.0M 0s\n",
      "  7450K .......... .......... .......... .......... .......... 37% 52.2M 0s\n",
      "  7500K .......... .......... .......... .......... .......... 37% 59.4M 0s\n",
      "  7550K .......... .......... .......... .......... .......... 37% 55.8M 0s\n",
      "  7600K .......... .......... .......... .......... .......... 38%  211M 0s\n",
      "  7650K .......... .......... .......... .......... .......... 38% 65.0M 0s\n",
      "  7700K .......... .......... .......... .......... .......... 38% 54.7M 0s\n",
      "  7750K .......... .......... .......... .......... .......... 38% 59.1M 0s\n",
      "  7800K .......... .......... .......... .......... .......... 39% 57.0M 0s\n",
      "  7850K .......... .......... .......... .......... .......... 39%  297M 0s\n",
      "  7900K .......... .......... .......... .......... .......... 39% 58.8M 0s\n",
      "  7950K .......... .......... .......... .......... .......... 39% 55.5M 0s\n",
      "  8000K .......... .......... .......... .......... .......... 40% 61.0M 0s\n",
      "  8050K .......... .......... .......... .......... .......... 40%  104M 0s\n",
      "  8100K .......... .......... .......... .......... .......... 40%  142M 0s\n",
      "  8150K .......... .......... .......... .......... .......... 40% 56.1M 0s\n",
      "  8200K .......... .......... .......... .......... .......... 41% 68.8M 0s\n",
      "  8250K .......... .......... .......... .......... .......... 41% 52.5M 0s\n",
      "  8300K .......... .......... .......... .......... .......... 41%  170M 0s\n",
      "  8350K .......... .......... .......... .......... .......... 41%  126M 0s\n",
      "  8400K .......... .......... .......... .......... .......... 42% 68.1M 0s\n",
      "  8450K .......... .......... .......... .......... .......... 42% 67.8M 0s\n",
      "  8500K .......... .......... .......... .......... .......... 42% 72.0M 0s\n",
      "  8550K .......... .......... .......... .......... .......... 42%  107M 0s\n",
      "  8600K .......... .......... .......... .......... .......... 43%  139M 0s\n",
      "  8650K .......... .......... .......... .......... .......... 43% 67.1M 0s\n",
      "  8700K .......... .......... .......... .......... .......... 43% 64.4M 0s\n",
      "  8750K .......... .......... .......... .......... .......... 43% 58.4M 0s\n",
      "  8800K .......... .......... .......... .......... .......... 44%  146M 0s\n",
      "  8850K .......... .......... .......... .......... .......... 44%  119M 0s\n",
      "  8900K .......... .......... .......... .......... .......... 44% 63.1M 0s\n",
      "  8950K .......... .......... .......... .......... .......... 44% 95.7M 0s\n",
      "  9000K .......... .......... .......... .......... .......... 45% 72.5M 0s\n",
      "  9050K .......... .......... .......... .......... .......... 45%  271M 0s\n",
      "  9100K .......... .......... .......... .......... .......... 45% 81.7M 0s\n",
      "  9150K .......... .......... .......... .......... .......... 45% 70.1M 0s\n",
      "  9200K .......... .......... .......... .......... .......... 46% 69.6M 0s\n",
      "  9250K .......... .......... .......... .......... .......... 46% 83.5M 0s\n",
      "  9300K .......... .......... .......... .......... .......... 46%  340M 0s\n",
      "  9350K .......... .......... .......... .......... .......... 46% 83.0M 0s\n",
      "  9400K .......... .......... .......... .......... .......... 47% 70.7M 0s\n",
      "  9450K .......... .......... .......... .......... .......... 47% 71.9M 0s\n",
      "  9500K .......... .......... .......... .......... .......... 47% 86.2M 0s\n",
      "  9550K .......... .......... .......... .......... .......... 47%  205M 0s\n",
      "  9600K .......... .......... .......... .......... .......... 48% 92.7M 0s\n",
      "  9650K .......... .......... .......... .......... .......... 48% 86.0M 0s\n",
      "  9700K .......... .......... .......... .......... .......... 48% 78.9M 0s\n",
      "  9750K .......... .......... .......... .......... .......... 48% 85.6M 0s\n",
      "  9800K .......... .......... .......... .......... .......... 49%  318M 0s\n",
      "  9850K .......... .......... .......... .......... .......... 49% 79.7M 0s\n",
      "  9900K .......... .......... .......... .......... .......... 49% 88.2M 0s\n",
      "  9950K .......... .......... .......... .......... .......... 49% 85.3M 0s\n",
      " 10000K .......... .......... .......... .......... .......... 50% 80.1M 0s\n",
      " 10050K .......... .......... .......... .......... .......... 50%  229M 0s\n",
      " 10100K .......... .......... .......... .......... .......... 50% 95.7M 0s\n",
      " 10150K .......... .......... .......... .......... .......... 50% 83.9M 0s\n",
      " 10200K .......... .......... .......... .......... .......... 51% 86.8M 0s\n",
      " 10250K .......... .......... .......... .......... .......... 51%  205M 0s\n",
      " 10300K .......... .......... .......... .......... .......... 51% 98.4M 0s\n",
      " 10350K .......... .......... .......... .......... .......... 51%  113M 0s\n",
      " 10400K .......... .......... .......... .......... .......... 52% 87.0M 0s\n",
      " 10450K .......... .......... .......... .......... .......... 52% 90.0M 0s\n",
      " 10500K .......... .......... .......... .......... .......... 52% 87.7M 0s\n",
      " 10550K .......... .......... .......... .......... .......... 52%  289M 0s\n",
      " 10600K .......... .......... .......... .......... .......... 53%  102M 0s\n",
      " 10650K .......... .......... .......... .......... .......... 53% 82.8M 0s\n",
      " 10700K .......... .......... .......... .......... .......... 53%  104M 0s\n",
      " 10750K .......... .......... .......... .......... .......... 53%  231M 0s\n",
      " 10800K .......... .......... .......... .......... .......... 54%  119M 0s\n",
      " 10850K .......... .......... .......... .......... .......... 54% 77.6M 0s\n",
      " 10900K .......... .......... .......... .......... .......... 54%  176M 0s\n",
      " 10950K .......... .......... .......... .......... .......... 54% 84.5M 0s\n",
      " 11000K .......... .......... .......... .......... .......... 55%  310M 0s\n",
      " 11050K .......... .......... .......... .......... .......... 55%  122M 0s\n",
      " 11100K .......... .......... .......... .......... .......... 55% 91.4M 0s\n",
      " 11150K .......... .......... .......... .......... .......... 55%  107M 0s\n",
      " 11200K .......... .......... .......... .......... .......... 56%  104M 0s\n",
      " 11250K .......... .......... .......... .......... .......... 56%  260M 0s\n",
      " 11300K .......... .......... .......... .......... .......... 56%  103M 0s\n",
      " 11350K .......... .......... .......... .......... .......... 56% 95.9M 0s\n",
      " 11400K .......... .......... .......... .......... .......... 57%  109M 0s\n",
      " 11450K .......... .......... .......... .......... .......... 57%  105M 0s\n",
      " 11500K .......... .......... .......... .......... .......... 57%  241M 0s\n",
      " 11550K .......... .......... .......... .......... .......... 57%  118M 0s\n",
      " 11600K .......... .......... .......... .......... .......... 58% 98.2M 0s\n",
      " 11650K .......... .......... .......... .......... .......... 58%  115M 0s\n",
      " 11700K .......... .......... .......... .......... .......... 58% 93.0M 0s\n",
      " 11750K .......... .......... .......... .......... .......... 58%  153M 0s\n",
      " 11800K .......... .......... .......... .......... .......... 59%  177M 0s\n",
      " 11850K .......... .......... .......... .......... .......... 59% 74.3M 0s\n",
      " 11900K .......... .......... .......... .......... .......... 59%  143M 0s\n",
      " 11950K .......... .......... .......... .......... .......... 59%  132M 0s\n",
      " 12000K .......... .......... .......... .......... .......... 60%  245M 0s\n",
      " 12050K .......... .......... .......... .......... .......... 60%  120M 0s\n",
      " 12100K .......... .......... .......... .......... .......... 60%  108M 0s\n",
      " 12150K .......... .......... .......... .......... .......... 60% 80.7M 0s\n",
      " 12200K .......... .......... .......... .......... .......... 61%  123M 0s\n",
      " 12250K .......... .......... .......... .......... .......... 61%  290M 0s\n",
      " 12300K .......... .......... .......... .......... .......... 61%  104M 0s\n",
      " 12350K .......... .......... .......... .......... .......... 61%  112M 0s\n",
      " 12400K .......... .......... .......... .......... .......... 62%  130M 0s\n",
      " 12450K .......... .......... .......... .......... .......... 62% 91.0M 0s\n",
      " 12500K .......... .......... .......... .......... .......... 62%  199M 0s\n",
      " 12550K .......... .......... .......... .......... .......... 62%  162M 0s\n",
      " 12600K .......... .......... .......... .......... .......... 63%  111M 0s\n",
      " 12650K .......... .......... .......... .......... .......... 63%  119M 0s\n",
      " 12700K .......... .......... .......... .......... .......... 63%  105M 0s\n",
      " 12750K .......... .......... .......... .......... .......... 63%  244M 0s\n",
      " 12800K .......... .......... .......... .......... .......... 64%  105M 0s\n",
      " 12850K .......... .......... .......... .......... .......... 64%  124M 0s\n",
      " 12900K .......... .......... .......... .......... .......... 64%  122M 0s\n",
      " 12950K .......... .......... .......... .......... .......... 64%  101M 0s\n",
      " 13000K .......... .......... .......... .......... .......... 65%  167M 0s\n",
      " 13050K .......... .......... .......... .......... .......... 65%  120M 0s\n",
      " 13100K .......... .......... .......... .......... .......... 65%  116M 0s\n",
      " 13150K .......... .......... .......... .......... .......... 65%  148M 0s\n",
      " 13200K .......... .......... .......... .......... .......... 66%  163M 0s\n",
      " 13250K .......... .......... .......... .......... .......... 66%  111M 0s\n",
      " 13300K .......... .......... .......... .......... .......... 66%  102M 0s\n",
      " 13350K .......... .......... .......... .......... .......... 66%  145M 0s\n",
      " 13400K .......... .......... .......... .......... .......... 67%  132M 0s\n",
      " 13450K .......... .......... .......... .......... .......... 67%  178M 0s\n",
      " 13500K .......... .......... .......... .......... .......... 67%  110M 0s\n",
      " 13550K .......... .......... .......... .......... .......... 67%  153M 0s\n",
      " 13600K .......... .......... .......... .......... .......... 68%  136M 0s\n",
      " 13650K .......... .......... .......... .......... .......... 68% 84.9M 0s\n",
      " 13700K .......... .......... .......... .......... .......... 68%  283M 0s\n",
      " 13750K .......... .......... .......... .......... .......... 68%  135M 0s\n",
      " 13800K .......... .......... .......... .......... .......... 69%  105M 0s\n",
      " 13850K .......... .......... .......... .......... .......... 69%  115M 0s\n",
      " 13900K .......... .......... .......... .......... .......... 69%  206M 0s\n",
      " 13950K .......... .......... .......... .......... .......... 69%  147M 0s\n",
      " 14000K .......... .......... .......... .......... .......... 70%  107M 0s\n",
      " 14050K .......... .......... .......... .......... .......... 70%  126M 0s\n",
      " 14100K .......... .......... .......... .......... .......... 70%  142M 0s\n",
      " 14150K .......... .......... .......... .......... .......... 70%  183M 0s\n",
      " 14200K .......... .......... .......... .......... .......... 71%  137M 0s\n",
      " 14250K .......... .......... .......... .......... .......... 71%  138M 0s\n",
      " 14300K .......... .......... .......... .......... .......... 71%  108M 0s\n",
      " 14350K .......... .......... .......... .......... .......... 71%  123M 0s\n",
      " 14400K .......... .......... .......... .......... .......... 72%  238M 0s\n",
      " 14450K .......... .......... .......... .......... .......... 72%  128M 0s\n",
      " 14500K .......... .......... .......... .......... .......... 72%  135M 0s\n",
      " 14550K .......... .......... .......... .......... .......... 72%  123M 0s\n",
      " 14600K .......... .......... .......... .......... .......... 73%  135M 0s\n",
      " 14650K .......... .......... .......... .......... .......... 73%  118M 0s\n",
      " 14700K .......... .......... .......... .......... .......... 73%  127M 0s\n",
      " 14750K .......... .......... .......... .......... .......... 73%  193M 0s\n",
      " 14800K .......... .......... .......... .......... .......... 74%  131M 0s\n",
      " 14850K .......... .......... .......... .......... .......... 74%  158M 0s\n",
      " 14900K .......... .......... .......... .......... .......... 74%  169M 0s\n",
      " 14950K .......... .......... .......... .......... .......... 74%  171M 0s\n",
      " 15000K .......... .......... .......... .......... .......... 75%  129M 0s\n",
      " 15050K .......... .......... .......... .......... .......... 75%  114M 0s\n",
      " 15100K .......... .......... .......... .......... .......... 75%  168M 0s\n",
      " 15150K .......... .......... .......... .......... .......... 75%  302M 0s\n",
      " 15200K .......... .......... .......... .......... .......... 76%  138M 0s\n",
      " 15250K .......... .......... .......... .......... .......... 76%  111M 0s\n",
      " 15300K .......... .......... .......... .......... .......... 76%  180M 0s\n",
      " 15350K .......... .......... .......... .......... .......... 76%  168M 0s\n",
      " 15400K .......... .......... .......... .......... .......... 77%  172M 0s\n",
      " 15450K .......... .......... .......... .......... .......... 77%  132M 0s\n",
      " 15500K .......... .......... .......... .......... .......... 77%  132M 0s\n",
      " 15550K .......... .......... .......... .......... .......... 77%  125M 0s\n",
      " 15600K .......... .......... .......... .......... .......... 78%  170M 0s\n",
      " 15650K .......... .......... .......... .......... .......... 78%  214M 0s\n",
      " 15700K .......... .......... .......... .......... .......... 78%  119M 0s\n",
      " 15750K .......... .......... .......... .......... .......... 78%  174M 0s\n",
      " 15800K .......... .......... .......... .......... .......... 79%  144M 0s\n",
      " 15850K .......... .......... .......... .......... .......... 79%  121M 0s\n",
      " 15900K .......... .......... .......... .......... .......... 79%  199M 0s\n",
      " 15950K .......... .......... .......... .......... .......... 79%  172M 0s\n",
      " 16000K .......... .......... .......... .......... .......... 80%  149M 0s\n",
      " 16050K .......... .......... .......... .......... .......... 80%  138M 0s\n",
      " 16100K .......... .......... .......... .......... .......... 80%  146M 0s\n",
      " 16150K .......... .......... .......... .......... .......... 80%  144M 0s\n",
      " 16200K .......... .......... .......... .......... .......... 81%  179M 0s\n",
      " 16250K .......... .......... .......... .......... .......... 81%  151M 0s\n",
      " 16300K .......... .......... .......... .......... .......... 81%  162M 0s\n",
      " 16350K .......... .......... .......... .......... .......... 81%  230M 0s\n",
      " 16400K .......... .......... .......... .......... .......... 82%  179M 0s\n",
      " 16450K .......... .......... .......... .......... .......... 82% 91.0M 0s\n",
      " 16500K .......... .......... .......... .......... .......... 82%  156M 0s\n",
      " 16550K .......... .......... .......... .......... .......... 82%  179M 0s\n",
      " 16600K .......... .......... .......... .......... .......... 83%  151M 0s\n",
      " 16650K .......... .......... .......... .......... .......... 83%  188M 0s\n",
      " 16700K .......... .......... .......... .......... .......... 83%  203M 0s\n",
      " 16750K .......... .......... .......... .......... .......... 83%  147M 0s\n",
      " 16800K .......... .......... .......... .......... .......... 84%  125M 0s\n",
      " 16850K .......... .......... .......... .......... .......... 84%  153M 0s\n",
      " 16900K .......... .......... .......... .......... .......... 84%  126M 0s\n",
      " 16950K .......... .......... .......... .......... .......... 84%  199M 0s\n",
      " 17000K .......... .......... .......... .......... .......... 85%  132M 0s\n",
      " 17050K .......... .......... .......... .......... .......... 85%  221M 0s\n",
      " 17100K .......... .......... .......... .......... .......... 85%  131M 0s\n",
      " 17150K .......... .......... .......... .......... .......... 85%  141M 0s\n",
      " 17200K .......... .......... .......... .......... .......... 86%  247M 0s\n",
      " 17250K .......... .......... .......... .......... .......... 86%  113M 0s\n",
      " 17300K .......... .......... .......... .......... .......... 86%  210M 0s\n",
      " 17350K .......... .......... .......... .......... .......... 86%  136M 0s\n",
      " 17400K .......... .......... .......... .......... .......... 87%  171M 0s\n",
      " 17450K .......... .......... .......... .......... .......... 87%  299M 0s\n",
      " 17500K .......... .......... .......... .......... .......... 87%  103M 0s\n",
      " 17550K .......... .......... .......... .......... .......... 87%  177M 0s\n",
      " 17600K .......... .......... .......... .......... .......... 88%  186M 0s\n",
      " 17650K .......... .......... .......... .......... .......... 88%  240M 0s\n",
      " 17700K .......... .......... .......... .......... .......... 88%  158M 0s\n",
      " 17750K .......... .......... .......... .......... .......... 88%  186M 0s\n",
      " 17800K .......... .......... .......... .......... .......... 89%  189M 0s\n",
      " 17850K .......... .......... .......... .......... .......... 89%  208M 0s\n",
      " 17900K .......... .......... .......... .......... .......... 89%  198M 0s\n",
      " 17950K .......... .......... .......... .......... .......... 89%  158M 0s\n",
      " 18000K .......... .......... .......... .......... .......... 90%  176M 0s\n",
      " 18050K .......... .......... .......... .......... .......... 90%  149M 0s\n",
      " 18100K .......... .......... .......... .......... .......... 90%  173M 0s\n",
      " 18150K .......... .......... .......... .......... .......... 90%  164M 0s\n",
      " 18200K .......... .......... .......... .......... .......... 91%  167M 0s\n",
      " 18250K .......... .......... .......... .......... .......... 91%  133M 0s\n",
      " 18300K .......... .......... .......... .......... .......... 91%  184M 0s\n",
      " 18350K .......... .......... .......... .......... .......... 91%  201M 0s\n",
      " 18400K .......... .......... .......... .......... .......... 92%  145M 0s\n",
      " 18450K .......... .......... .......... .......... .......... 92%  173M 0s\n",
      " 18500K .......... .......... .......... .......... .......... 92%  156M 0s\n",
      " 18550K .......... .......... .......... .......... .......... 92%  198M 0s\n",
      " 18600K .......... .......... .......... .......... .......... 93%  164M 0s\n",
      " 18650K .......... .......... .......... .......... .......... 93%  246M 0s\n",
      " 18700K .......... .......... .......... .......... .......... 93%  119M 0s\n",
      " 18750K .......... .......... .......... .......... .......... 93%  173M 0s\n",
      " 18800K .......... .......... .......... .......... .......... 94%  233M 0s\n",
      " 18850K .......... .......... .......... .......... .......... 94%  175M 0s\n",
      " 18900K .......... .......... .......... .......... .......... 94%  155M 0s\n",
      " 18950K .......... .......... .......... .......... .......... 94%  151M 0s\n",
      " 19000K .......... .......... .......... .......... .......... 95%  173M 0s\n",
      " 19050K .......... .......... .......... .......... .......... 95%  173M 0s\n",
      " 19100K .......... .......... .......... .......... .......... 95%  196M 0s\n",
      " 19150K .......... .......... .......... .......... .......... 95%  116M 0s\n",
      " 19200K .......... .......... .......... .......... .......... 96%  196M 0s\n",
      " 19250K .......... .......... .......... .......... .......... 96%  166M 0s\n",
      " 19300K .......... .......... .......... .......... .......... 96%  201M 0s\n",
      " 19350K .......... .......... .......... .......... .......... 96%  159M 0s\n",
      " 19400K .......... .......... .......... .......... .......... 97%  197M 0s\n",
      " 19450K .......... .......... .......... .......... .......... 97%  179M 0s\n",
      " 19500K .......... .......... .......... .......... .......... 97%  149M 0s\n",
      " 19550K .......... .......... .......... .......... .......... 97%  150M 0s\n",
      " 19600K .......... .......... .......... .......... .......... 98%  162M 0s\n",
      " 19650K .......... .......... .......... .......... .......... 98%  153M 0s\n",
      " 19700K .......... .......... .......... .......... .......... 98%  204M 0s\n",
      " 19750K .......... .......... .......... .......... .......... 98%  235M 0s\n",
      " 19800K .......... .......... .......... .......... .......... 99%  286M 0s\n",
      " 19850K .......... .......... .......... .......... .......... 99%  313M 0s\n",
      " 19900K .......... .......... .......... .......... .......... 99%  317M 0s\n",
      " 19950K .......... .......... .......... .......... .......... 99%  234M 0s\n",
      " 20000K .......... .......... .......... .......              100%  316M=0.3s\n",
      "\n",
      "2025-10-02 01:34:41 (73.5 MB/s) - ‘/tmp/efficientdet.tar.gz’ saved [20518283/20518283]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/checkpoint/ckpt-0.data-00000-of-00001\n",
      "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/checkpoint/checkpoint\n",
      "ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/checkpoint/ckpt-0.index\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir /tmp/checkpoint\n",
    "mkdir source_dir/checkpoint\n",
    "wget -O /tmp/efficientdet.tar.gz http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz\n",
    "tar -zxvf /tmp/efficientdet.tar.gz --strip-components 2 --directory source_dir/checkpoint ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1e04a98",
   "metadata": {},
   "source": [
    "## Edit pipeline.config file\n",
    "\n",
    "The [`pipeline.config`](source_dir/pipeline.config) in the `source_dir` folder should be updated when you experiment with different models. The different config files are available [here](https://github.com/tensorflow/models/tree/master/research/object_detection/configs/tf2).\n",
    "\n",
    ">Note: The provided `pipeline.config` file works well with the `EfficientDet` model. You would need to modify it when working with other models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47483545",
   "metadata": {},
   "source": [
    "## Launch Training Job\n",
    "\n",
    "Now that we have a dataset, a docker image and some pretrained model weights, we can launch the training job. To do so, we create a [Sagemaker Framework](https://sagemaker.readthedocs.io/en/stable/frameworks/index.html), where we indicate the container name, name of the config file, number of training steps etc.\n",
    "\n",
    "The `run_training.sh` script does the following:\n",
    "* train the model for `num_train_steps` \n",
    "* evaluate over the val dataset\n",
    "* export the model\n",
    "\n",
    "Different metrics will be displayed during the evaluation phase, including the mean average precision. These metrics can be used to quantify your model performances and compare over the different iterations.\n",
    "\n",
    "You can also monitor the training progress by navigating to **Training -> Training Jobs** from the Amazon Sagemaker dashboard in the Web UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c7175cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: tf2-object-detection-2025-10-02-01-34-41-965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-02 01:34:43 Starting - Starting the training job\n",
      "2025-10-02 01:34:43 Pending - Training job waiting for capacity.........\n",
      "2025-10-02 01:36:04 Pending - Preparing the instances for training...\n",
      "2025-10-02 01:36:36 Downloading - Downloading input data...\n",
      "2025-10-02 01:36:51 Downloading - Downloading the training image.........\n",
      "2025-10-02 01:38:43 Training - Training image download completed. Training in progress...\u001b[34m2025-10-02 01:38:57,796 sagemaker-training-toolkit INFO     Provided path: /opt/ml/code  is empty, unzipping\u001b[0m\n",
      "\u001b[34m2025-10-02 01:38:58,410 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-10-02 01:38:58,445 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-10-02 01:38:58,481 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-10-02 01:38:58,494 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"val\": \"/opt/ml/input/data/val\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": null,\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_dir\": \"/opt/training\",\n",
      "        \"num_train_steps\": \"2000\",\n",
      "        \"pipeline_config_path\": \"pipeline.config\",\n",
      "        \"sample_1_of_n_eval_examples\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"tf2-object-detection-2025-10-02-01-34-41-965\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-738138308218/tf2-object-detection-2025-10-02-01-34-41-965/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_training.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"topology\": null,\n",
      "    \"user_entry_point\": \"run_training.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"model_dir\":\"/opt/training\",\"num_train_steps\":\"2000\",\"pipeline_config_path\":\"pipeline.config\",\"sample_1_of_n_eval_examples\":\"1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_training.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_training.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-738138308218/tf2-object-detection-2025-10-02-01-34-41-965/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":null,\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"model_dir\":\"/opt/training\",\"num_train_steps\":\"2000\",\"pipeline_config_path\":\"pipeline.config\",\"sample_1_of_n_eval_examples\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"tf2-object-detection-2025-10-02-01-34-41-965\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-738138308218/tf2-object-detection-2025-10-02-01-34-41-965/source/sourcedir.tar.gz\",\"module_name\":\"run_training.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"topology\":null,\"user_entry_point\":\"run_training.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--model_dir\",\"/opt/training\",\"--num_train_steps\",\"2000\",\"--pipeline_config_path\",\"pipeline.config\",\"--sample_1_of_n_eval_examples\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/training\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_STEPS=2000\u001b[0m\n",
      "\u001b[34mSM_HP_PIPELINE_CONFIG_PATH=pipeline.config\u001b[0m\n",
      "\u001b[34mSM_HP_SAMPLE_1_OF_N_EVAL_EXAMPLES=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python38.zip:/usr/lib/python3.8:/usr/lib/python3.8/lib-dynload:/usr/local/lib/python3.8/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./run_training.sh --model_dir /opt/training --num_train_steps 2000 --pipeline_config_path pipeline.config --sample_1_of_n_eval_examples 1\"\u001b[0m\n",
      "\u001b[34m2025-10-02 01:38:58,494 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m===TRAINING THE MODEL==\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\u001b[0m\n",
      "\u001b[34mI1002 01:39:05.262759 140702959593280 mirrored_strategy.py:419] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Maybe overwriting train_steps: 2000\u001b[0m\n",
      "\u001b[34mI1002 01:39:05.537828 140702959593280 config_util.py:552] Maybe overwriting train_steps: 2000\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Maybe overwriting use_bfloat16: False\u001b[0m\n",
      "\u001b[34mI1002 01:39:05.537978 140702959593280 config_util.py:552] Maybe overwriting use_bfloat16: False\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mrename to distribute_datasets_from_function\u001b[0m\n",
      "\u001b[34mW1002 01:39:05.559942 140702959593280 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mrename to distribute_datasets_from_function\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reading unweighted datasets: ['/opt/ml/input/data/train/*.tfrecord']\u001b[0m\n",
      "\u001b[34mI1002 01:39:05.565729 140702959593280 dataset_builder.py:162] Reading unweighted datasets: ['/opt/ml/input/data/train/*.tfrecord']\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reading record datasets for input file: ['/opt/ml/input/data/train/*.tfrecord']\u001b[0m\n",
      "\u001b[34mI1002 01:39:05.566867 140702959593280 dataset_builder.py:79] Reading record datasets for input file: ['/opt/ml/input/data/train/*.tfrecord']\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Number of filenames to read: 84\u001b[0m\n",
      "\u001b[34mI1002 01:39:05.566946 140702959593280 dataset_builder.py:80] Number of filenames to read: 84\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\u001b[0m\n",
      "\u001b[34mW1002 01:39:05.572540 140702959593280 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.map()\u001b[0m\n",
      "\u001b[34mW1002 01:39:05.591073 140702959593280 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.map()\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\u001b[0m\n",
      "\u001b[34mW1002 01:39:11.118312 140702959593280 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34m`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\u001b[0m\n",
      "\u001b[34mW1002 01:39:13.525981 140702959593280 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34m`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.cast` instead.\u001b[0m\n",
      "\u001b[34mW1002 01:39:14.741776 140702959593280 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.cast` instead.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/keras/src/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mI1002 01:39:25.469452 140674511177472 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mI1002 01:39:33.132782 140674511177472 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mI1002 01:39:39.929849 140702959593280 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mI1002 01:39:39.931984 140702959593280 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mI1002 01:39:39.932742 140702959593280 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mI1002 01:39:39.933427 140702959593280 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mI1002 01:39:39.936075 140702959593280 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mI1002 01:39:39.936740 140702959593280 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mI1002 01:39:39.937482 140702959593280 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mI1002 01:39:39.938122 140702959593280 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mI1002 01:39:39.941485 140702959593280 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mI1002 01:39:39.942164 140702959593280 cross_device_ops.py:617] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/deprecation.py:648: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse fn_output_signature instead\u001b[0m\n",
      "\u001b[34mW1002 01:39:41.306530 140674511177472 deprecation.py:569] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/deprecation.py:648: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse fn_output_signature instead\u001b[0m\n",
      "\u001b[34mI1002 01:39:42.315270 140674511177472 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mI1002 01:39:48.527452 140674511177472 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mI1002 01:39:53.770102 140674511177472 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mI1002 01:39:59.473646 140674511177472 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 100 per-step time 0.610s\u001b[0m\n",
      "\u001b[34mI1002 01:40:41.626953 140702959593280 model_lib_v2.py:705] Step 100 per-step time 0.610s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.4314815,\n",
      " 'Loss/localization_loss': 0.46703887,\n",
      " 'Loss/regularization_loss': 0.15143101,\n",
      " 'Loss/total_loss': 1.0499513,\n",
      " 'learning_rate': 0.0319994}\u001b[0m\n",
      "\u001b[34mI1002 01:40:41.627247 140702959593280 model_lib_v2.py:708] {'Loss/classification_loss': 0.4314815,\n",
      " 'Loss/localization_loss': 0.46703887,\n",
      " 'Loss/regularization_loss': 0.15143101,\n",
      " 'Loss/total_loss': 1.0499513,\n",
      " 'learning_rate': 0.0319994}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 200 per-step time 0.263s\u001b[0m\n",
      "\u001b[34mI1002 01:41:07.845428 140702959593280 model_lib_v2.py:705] Step 200 per-step time 0.263s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.2657014,\n",
      " 'Loss/localization_loss': 0.40211403,\n",
      " 'Loss/regularization_loss': 0.15140769,\n",
      " 'Loss/total_loss': 0.81922317,\n",
      " 'learning_rate': 0.0373328}\u001b[0m\n",
      "\u001b[34mI1002 01:41:07.845660 140702959593280 model_lib_v2.py:708] {'Loss/classification_loss': 0.2657014,\n",
      " 'Loss/localization_loss': 0.40211403,\n",
      " 'Loss/regularization_loss': 0.15140769,\n",
      " 'Loss/total_loss': 0.81922317,\n",
      " 'learning_rate': 0.0373328}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 300 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mI1002 01:41:34.073793 140702959593280 model_lib_v2.py:705] Step 300 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.3207326,\n",
      " 'Loss/localization_loss': 0.36898214,\n",
      " 'Loss/regularization_loss': 0.15137076,\n",
      " 'Loss/total_loss': 0.8410855,\n",
      " 'learning_rate': 0.0426662}\u001b[0m\n",
      "\u001b[34mI1002 01:41:34.074040 140702959593280 model_lib_v2.py:708] {'Loss/classification_loss': 0.3207326,\n",
      " 'Loss/localization_loss': 0.36898214,\n",
      " 'Loss/regularization_loss': 0.15137076,\n",
      " 'Loss/total_loss': 0.8410855,\n",
      " 'learning_rate': 0.0426662}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 400 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mI1002 01:42:00.305927 140702959593280 model_lib_v2.py:705] Step 400 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.30735213,\n",
      " 'Loss/localization_loss': 0.40572473,\n",
      " 'Loss/regularization_loss': 0.15130126,\n",
      " 'Loss/total_loss': 0.8643781,\n",
      " 'learning_rate': 0.047999598}\u001b[0m\n",
      "\u001b[34mI1002 01:42:00.306142 140702959593280 model_lib_v2.py:708] {'Loss/classification_loss': 0.30735213,\n",
      " 'Loss/localization_loss': 0.40572473,\n",
      " 'Loss/regularization_loss': 0.15130126,\n",
      " 'Loss/total_loss': 0.8643781,\n",
      " 'learning_rate': 0.047999598}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 500 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mI1002 01:42:26.517195 140702959593280 model_lib_v2.py:705] Step 500 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.29352584,\n",
      " 'Loss/localization_loss': 0.33273652,\n",
      " 'Loss/regularization_loss': 0.15116292,\n",
      " 'Loss/total_loss': 0.7774253,\n",
      " 'learning_rate': 0.053333}\u001b[0m\n",
      "\u001b[34mI1002 01:42:26.517415 140702959593280 model_lib_v2.py:708] {'Loss/classification_loss': 0.29352584,\n",
      " 'Loss/localization_loss': 0.33273652,\n",
      " 'Loss/regularization_loss': 0.15116292,\n",
      " 'Loss/total_loss': 0.7774253,\n",
      " 'learning_rate': 0.053333}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 600 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mI1002 01:42:52.741444 140702959593280 model_lib_v2.py:705] Step 600 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.28944984,\n",
      " 'Loss/localization_loss': 0.32199717,\n",
      " 'Loss/regularization_loss': 0.15112554,\n",
      " 'Loss/total_loss': 0.7625725,\n",
      " 'learning_rate': 0.0586664}\u001b[0m\n",
      "\u001b[34mI1002 01:42:52.741678 140702959593280 model_lib_v2.py:708] {'Loss/classification_loss': 0.28944984,\n",
      " 'Loss/localization_loss': 0.32199717,\n",
      " 'Loss/regularization_loss': 0.15112554,\n",
      " 'Loss/total_loss': 0.7625725,\n",
      " 'learning_rate': 0.0586664}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 700 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mI1002 01:43:18.945802 140702959593280 model_lib_v2.py:705] Step 700 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.2617973,\n",
      " 'Loss/localization_loss': 0.34180492,\n",
      " 'Loss/regularization_loss': 0.15106115,\n",
      " 'Loss/total_loss': 0.75466335,\n",
      " 'learning_rate': 0.0639998}\u001b[0m\n",
      "\u001b[34mI1002 01:43:18.946023 140702959593280 model_lib_v2.py:708] {'Loss/classification_loss': 0.2617973,\n",
      " 'Loss/localization_loss': 0.34180492,\n",
      " 'Loss/regularization_loss': 0.15106115,\n",
      " 'Loss/total_loss': 0.75466335,\n",
      " 'learning_rate': 0.0639998}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 800 per-step time 0.263s\u001b[0m\n",
      "\u001b[34mI1002 01:43:45.207746 140702959593280 model_lib_v2.py:705] Step 800 per-step time 0.263s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.22452617,\n",
      " 'Loss/localization_loss': 0.2689151,\n",
      " 'Loss/regularization_loss': 0.15100852,\n",
      " 'Loss/total_loss': 0.6444498,\n",
      " 'learning_rate': 0.069333196}\u001b[0m\n",
      "\u001b[34mI1002 01:43:45.207973 140702959593280 model_lib_v2.py:708] {'Loss/classification_loss': 0.22452617,\n",
      " 'Loss/localization_loss': 0.2689151,\n",
      " 'Loss/regularization_loss': 0.15100852,\n",
      " 'Loss/total_loss': 0.6444498,\n",
      " 'learning_rate': 0.069333196}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 900 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mI1002 01:44:11.425808 140702959593280 model_lib_v2.py:705] Step 900 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.28823504,\n",
      " 'Loss/localization_loss': 0.2744399,\n",
      " 'Loss/regularization_loss': 0.1510119,\n",
      " 'Loss/total_loss': 0.7136868,\n",
      " 'learning_rate': 0.074666604}\u001b[0m\n",
      "\u001b[34mI1002 01:44:11.426030 140702959593280 model_lib_v2.py:708] {'Loss/classification_loss': 0.28823504,\n",
      " 'Loss/localization_loss': 0.2744399,\n",
      " 'Loss/regularization_loss': 0.1510119,\n",
      " 'Loss/total_loss': 0.7136868,\n",
      " 'learning_rate': 0.074666604}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1000 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mI1002 01:44:37.655143 140702959593280 model_lib_v2.py:705] Step 1000 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.22174503,\n",
      " 'Loss/localization_loss': 0.23123293,\n",
      " 'Loss/regularization_loss': 0.15101248,\n",
      " 'Loss/total_loss': 0.60399044,\n",
      " 'learning_rate': 0.08}\u001b[0m\n",
      "\u001b[34mI1002 01:44:37.655373 140702959593280 model_lib_v2.py:708] {'Loss/classification_loss': 0.22174503,\n",
      " 'Loss/localization_loss': 0.23123293,\n",
      " 'Loss/regularization_loss': 0.15101248,\n",
      " 'Loss/total_loss': 0.60399044,\n",
      " 'learning_rate': 0.08}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1100 per-step time 0.276s\u001b[0m\n",
      "\u001b[34mI1002 01:45:05.228034 140702959593280 model_lib_v2.py:705] Step 1100 per-step time 0.276s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.23682259,\n",
      " 'Loss/localization_loss': 0.32890293,\n",
      " 'Loss/regularization_loss': 0.1509304,\n",
      " 'Loss/total_loss': 0.7166559,\n",
      " 'learning_rate': 0.07999918}\u001b[0m\n",
      "\u001b[34mI1002 01:45:05.228273 140702959593280 model_lib_v2.py:708] {'Loss/classification_loss': 0.23682259,\n",
      " 'Loss/localization_loss': 0.32890293,\n",
      " 'Loss/regularization_loss': 0.1509304,\n",
      " 'Loss/total_loss': 0.7166559,\n",
      " 'learning_rate': 0.07999918}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1200 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mI1002 01:45:31.434147 140702959593280 model_lib_v2.py:705] Step 1200 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.21798119,\n",
      " 'Loss/localization_loss': 0.27553585,\n",
      " 'Loss/regularization_loss': 0.15067878,\n",
      " 'Loss/total_loss': 0.6441958,\n",
      " 'learning_rate': 0.079996705}\u001b[0m\n",
      "\u001b[34mI1002 01:45:31.434367 140702959593280 model_lib_v2.py:708] {'Loss/classification_loss': 0.21798119,\n",
      " 'Loss/localization_loss': 0.27553585,\n",
      " 'Loss/regularization_loss': 0.15067878,\n",
      " 'Loss/total_loss': 0.6441958,\n",
      " 'learning_rate': 0.079996705}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1300 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mI1002 01:45:57.661393 140702959593280 model_lib_v2.py:705] Step 1300 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.20444584,\n",
      " 'Loss/localization_loss': 0.23958981,\n",
      " 'Loss/regularization_loss': 0.15051423,\n",
      " 'Loss/total_loss': 0.5945499,\n",
      " 'learning_rate': 0.0799926}\u001b[0m\n",
      "\u001b[34mI1002 01:45:57.661616 140702959593280 model_lib_v2.py:708] {'Loss/classification_loss': 0.20444584,\n",
      " 'Loss/localization_loss': 0.23958981,\n",
      " 'Loss/regularization_loss': 0.15051423,\n",
      " 'Loss/total_loss': 0.5945499,\n",
      " 'learning_rate': 0.0799926}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1400 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mI1002 01:46:23.896358 140702959593280 model_lib_v2.py:705] Step 1400 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.2048598,\n",
      " 'Loss/localization_loss': 0.2520691,\n",
      " 'Loss/regularization_loss': 0.1503816,\n",
      " 'Loss/total_loss': 0.6073105,\n",
      " 'learning_rate': 0.07998685}\u001b[0m\n",
      "\u001b[34mI1002 01:46:23.896629 140702959593280 model_lib_v2.py:708] {'Loss/classification_loss': 0.2048598,\n",
      " 'Loss/localization_loss': 0.2520691,\n",
      " 'Loss/regularization_loss': 0.1503816,\n",
      " 'Loss/total_loss': 0.6073105,\n",
      " 'learning_rate': 0.07998685}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1500 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mI1002 01:46:50.096519 140702959593280 model_lib_v2.py:705] Step 1500 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.22638637,\n",
      " 'Loss/localization_loss': 0.28075418,\n",
      " 'Loss/regularization_loss': 0.150216,\n",
      " 'Loss/total_loss': 0.6573565,\n",
      " 'learning_rate': 0.07997945}\u001b[0m\n",
      "\u001b[34mI1002 01:46:50.096735 140702959593280 model_lib_v2.py:708] {'Loss/classification_loss': 0.22638637,\n",
      " 'Loss/localization_loss': 0.28075418,\n",
      " 'Loss/regularization_loss': 0.150216,\n",
      " 'Loss/total_loss': 0.6573565,\n",
      " 'learning_rate': 0.07997945}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1600 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mI1002 01:47:16.311421 140702959593280 model_lib_v2.py:705] Step 1600 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.17425522,\n",
      " 'Loss/localization_loss': 0.22292475,\n",
      " 'Loss/regularization_loss': 0.14995855,\n",
      " 'Loss/total_loss': 0.5471385,\n",
      " 'learning_rate': 0.079970405}\u001b[0m\n",
      "\u001b[34mI1002 01:47:16.311639 140702959593280 model_lib_v2.py:708] {'Loss/classification_loss': 0.17425522,\n",
      " 'Loss/localization_loss': 0.22292475,\n",
      " 'Loss/regularization_loss': 0.14995855,\n",
      " 'Loss/total_loss': 0.5471385,\n",
      " 'learning_rate': 0.079970405}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1700 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mI1002 01:47:42.545291 140702959593280 model_lib_v2.py:705] Step 1700 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.18177253,\n",
      " 'Loss/localization_loss': 0.25761265,\n",
      " 'Loss/regularization_loss': 0.14968073,\n",
      " 'Loss/total_loss': 0.5890659,\n",
      " 'learning_rate': 0.07995972}\u001b[0m\n",
      "\u001b[34mI1002 01:47:42.545525 140702959593280 model_lib_v2.py:708] {'Loss/classification_loss': 0.18177253,\n",
      " 'Loss/localization_loss': 0.25761265,\n",
      " 'Loss/regularization_loss': 0.14968073,\n",
      " 'Loss/total_loss': 0.5890659,\n",
      " 'learning_rate': 0.07995972}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1800 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mI1002 01:48:08.758954 140702959593280 model_lib_v2.py:705] Step 1800 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.20752233,\n",
      " 'Loss/localization_loss': 0.3080032,\n",
      " 'Loss/regularization_loss': 0.14945348,\n",
      " 'Loss/total_loss': 0.664979,\n",
      " 'learning_rate': 0.0799474}\u001b[0m\n",
      "\u001b[34mI1002 01:48:08.759173 140702959593280 model_lib_v2.py:708] {'Loss/classification_loss': 0.20752233,\n",
      " 'Loss/localization_loss': 0.3080032,\n",
      " 'Loss/regularization_loss': 0.14945348,\n",
      " 'Loss/total_loss': 0.664979,\n",
      " 'learning_rate': 0.0799474}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 1900 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mI1002 01:48:34.990422 140702959593280 model_lib_v2.py:705] Step 1900 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.1774119,\n",
      " 'Loss/localization_loss': 0.25541845,\n",
      " 'Loss/regularization_loss': 0.14916272,\n",
      " 'Loss/total_loss': 0.58199304,\n",
      " 'learning_rate': 0.07993342}\u001b[0m\n",
      "\u001b[34mI1002 01:48:34.990643 140702959593280 model_lib_v2.py:708] {'Loss/classification_loss': 0.1774119,\n",
      " 'Loss/localization_loss': 0.25541845,\n",
      " 'Loss/regularization_loss': 0.14916272,\n",
      " 'Loss/total_loss': 0.58199304,\n",
      " 'learning_rate': 0.07993342}\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Step 2000 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mI1002 01:49:01.194743 140702959593280 model_lib_v2.py:705] Step 2000 per-step time 0.262s\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:{'Loss/classification_loss': 0.1926817,\n",
      " 'Loss/localization_loss': 0.26286995,\n",
      " 'Loss/regularization_loss': 0.1488453,\n",
      " 'Loss/total_loss': 0.60439694,\n",
      " 'learning_rate': 0.07991781}\u001b[0m\n",
      "\u001b[34mI1002 01:49:01.194983 140702959593280 model_lib_v2.py:708] {'Loss/classification_loss': 0.1926817,\n",
      " 'Loss/localization_loss': 0.26286995,\n",
      " 'Loss/regularization_loss': 0.1488453,\n",
      " 'Loss/total_loss': 0.60439694,\n",
      " 'learning_rate': 0.07991781}\u001b[0m\n",
      "\u001b[34m==EVALUATING THE MODEL==\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\u001b[0m\n",
      "\u001b[34mW1002 01:49:07.905689 140144583157568 model_lib_v2.py:1089] Forced number of epochs for all eval validations to be 1.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: None\u001b[0m\n",
      "\u001b[34mI1002 01:49:07.905845 140144583157568 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: None\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Maybe overwriting use_bfloat16: False\u001b[0m\n",
      "\u001b[34mI1002 01:49:07.905921 140144583157568 config_util.py:552] Maybe overwriting use_bfloat16: False\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Maybe overwriting eval_num_epochs: 1\u001b[0m\n",
      "\u001b[34mI1002 01:49:07.905997 140144583157568 config_util.py:552] Maybe overwriting eval_num_epochs: 1\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\u001b[0m\n",
      "\u001b[34mW1002 01:49:07.906133 140144583157568 model_lib_v2.py:1106] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reading unweighted datasets: ['/opt/ml/input/data/val/*.tfrecord']\u001b[0m\n",
      "\u001b[34mI1002 01:49:08.268007 140144583157568 dataset_builder.py:162] Reading unweighted datasets: ['/opt/ml/input/data/val/*.tfrecord']\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Reading record datasets for input file: ['/opt/ml/input/data/val/*.tfrecord']\u001b[0m\n",
      "\u001b[34mI1002 01:49:08.268908 140144583157568 dataset_builder.py:79] Reading record datasets for input file: ['/opt/ml/input/data/val/*.tfrecord']\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Number of filenames to read: 13\u001b[0m\n",
      "\u001b[34mI1002 01:49:08.269006 140144583157568 dataset_builder.py:80] Number of filenames to read: 13\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:num_readers has been reduced to 13 to match input file shards.\u001b[0m\n",
      "\u001b[34mW1002 01:49:08.269091 140144583157568 dataset_builder.py:86] num_readers has been reduced to 13 to match input file shards.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:`shuffle` is false, but the input data stream is still slightly shuffled since `num_readers` > 1.\u001b[0m\n",
      "\u001b[34mW1002 01:49:08.270743 140144583157568 dataset_builder.py:93] `shuffle` is false, but the input data stream is still slightly shuffled since `num_readers` > 1.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\u001b[0m\n",
      "\u001b[34mW1002 01:49:08.272107 140144583157568 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.map()\u001b[0m\n",
      "\u001b[34mW1002 01:49:08.290303 140144583157568 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.data.Dataset.map()\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\u001b[0m\n",
      "\u001b[34mW1002 01:49:11.707377 140144583157568 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.cast` instead.\u001b[0m\n",
      "\u001b[34mW1002 01:49:12.493542 140144583157568 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.cast` instead.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Waiting for new checkpoint at /opt/training\u001b[0m\n",
      "\u001b[34mI1002 01:49:14.969074 140144583157568 checkpoint_utils.py:168] Waiting for new checkpoint at /opt/training\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Found new checkpoint at /opt/training/ckpt-3\u001b[0m\n",
      "\u001b[34mI1002 01:49:14.969672 140144583157568 checkpoint_utils.py:177] Found new checkpoint at /opt/training/ckpt-3\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/keras/src/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mI1002 01:49:19.676747 140144583157568 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mI1002 01:49:38.875021 140144583157568 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.cast` instead.\u001b[0m\n",
      "\u001b[34mW1002 01:49:54.071229 140144583157568 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mUse `tf.cast` instead.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Finished eval step 0\u001b[0m\n",
      "\u001b[34mI1002 01:49:54.110291 140144583157568 model_lib_v2.py:966] Finished eval step 0\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:460: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mtf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \u001b[0m\n",
      "\u001b[34mW1002 01:49:54.224233 140144583157568 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:460: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mtf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Finished eval step 100\u001b[0m\n",
      "\u001b[34mI1002 01:50:09.397242 140144583157568 model_lib_v2.py:966] Finished eval step 100\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Finished eval step 200\u001b[0m\n",
      "\u001b[34mI1002 01:50:21.653919 140144583157568 model_lib_v2.py:966] Finished eval step 200\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Performing evaluation on 258 images.\u001b[0m\n",
      "\u001b[34mI1002 01:50:28.639836 140144583157568 coco_evaluation.py:293] Performing evaluation on 258 images.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Loading and preparing annotation results...\u001b[0m\n",
      "\u001b[34mI1002 01:50:28.643826 140144583157568 coco_tools.py:116] Loading and preparing annotation results...\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:DONE (t=0.01s)\u001b[0m\n",
      "\u001b[34mI1002 01:50:28.655779 140144583157568 coco_tools.py:138] DONE (t=0.01s)\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Eval metrics at step 2000\u001b[0m\n",
      "\u001b[34mI1002 01:50:34.581693 140144583157568 model_lib_v2.py:1015] Eval metrics at step 2000\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Precision/mAP: 0.060770\u001b[0m\n",
      "\u001b[34mI1002 01:50:34.599951 140144583157568 model_lib_v2.py:1018] #011+ DetectionBoxes_Precision/mAP: 0.060770\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Precision/mAP@.50IOU: 0.132082\u001b[0m\n",
      "\u001b[34mI1002 01:50:34.601434 140144583157568 model_lib_v2.py:1018] #011+ DetectionBoxes_Precision/mAP@.50IOU: 0.132082\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Precision/mAP@.75IOU: 0.053454\u001b[0m\n",
      "\u001b[34mI1002 01:50:34.602435 140144583157568 model_lib_v2.py:1018] #011+ DetectionBoxes_Precision/mAP@.75IOU: 0.053454\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Precision/mAP (small): 0.023326\u001b[0m\n",
      "\u001b[34mI1002 01:50:34.603458 140144583157568 model_lib_v2.py:1018] #011+ DetectionBoxes_Precision/mAP (small): 0.023326\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Precision/mAP (medium): 0.212973\u001b[0m\n",
      "\u001b[34mI1002 01:50:34.604496 140144583157568 model_lib_v2.py:1018] #011+ DetectionBoxes_Precision/mAP (medium): 0.212973\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Precision/mAP (large): 0.387561\u001b[0m\n",
      "\u001b[34mI1002 01:50:34.605551 140144583157568 model_lib_v2.py:1018] #011+ DetectionBoxes_Precision/mAP (large): 0.387561\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Recall/AR@1: 0.020977\u001b[0m\n",
      "\u001b[34mI1002 01:50:34.606626 140144583157568 model_lib_v2.py:1018] #011+ DetectionBoxes_Recall/AR@1: 0.020977\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Recall/AR@10: 0.074906\u001b[0m\n",
      "\u001b[34mI1002 01:50:34.607664 140144583157568 model_lib_v2.py:1018] #011+ DetectionBoxes_Recall/AR@10: 0.074906\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Recall/AR@100: 0.108073\u001b[0m\n",
      "\u001b[34mI1002 01:50:34.608701 140144583157568 model_lib_v2.py:1018] #011+ DetectionBoxes_Recall/AR@100: 0.108073\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Recall/AR@100 (small): 0.059640\u001b[0m\n",
      "\u001b[34mI1002 01:50:34.609736 140144583157568 model_lib_v2.py:1018] #011+ DetectionBoxes_Recall/AR@100 (small): 0.059640\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Recall/AR@100 (medium): 0.377676\u001b[0m\n",
      "\u001b[34mI1002 01:50:34.610775 140144583157568 model_lib_v2.py:1018] #011+ DetectionBoxes_Recall/AR@100 (medium): 0.377676\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ DetectionBoxes_Recall/AR@100 (large): 0.525380\u001b[0m\n",
      "\u001b[34mI1002 01:50:34.611848 140144583157568 model_lib_v2.py:1018] #011+ DetectionBoxes_Recall/AR@100 (large): 0.525380\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ Loss/localization_loss: 0.434459\u001b[0m\n",
      "\u001b[34mI1002 01:50:34.612699 140144583157568 model_lib_v2.py:1018] #011+ Loss/localization_loss: 0.434459\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ Loss/classification_loss: 0.365183\u001b[0m\n",
      "\u001b[34mI1002 01:50:34.613539 140144583157568 model_lib_v2.py:1018] #011+ Loss/classification_loss: 0.365183\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ Loss/regularization_loss: 0.148843\u001b[0m\n",
      "\u001b[34mI1002 01:50:34.614324 140144583157568 model_lib_v2.py:1018] #011+ Loss/regularization_loss: 0.148843\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:#011+ Loss/total_loss: 0.948485\u001b[0m\n",
      "\u001b[34mI1002 01:50:34.615174 140144583157568 model_lib_v2.py:1018] #011+ Loss/total_loss: 0.948485\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Waiting for new checkpoint at /opt/training\u001b[0m\n",
      "\u001b[34mI1002 01:54:15.048329 140144583157568 checkpoint_utils.py:168] Waiting for new checkpoint at /opt/training\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Timed-out waiting for a checkpoint.\u001b[0m\n",
      "\u001b[34mI1002 01:54:24.062739 140144583157568 checkpoint_utils.py:231] Timed-out waiting for a checkpoint.\u001b[0m\n",
      "\u001b[34mcreating index...\u001b[0m\n",
      "\u001b[34mindex created!\u001b[0m\n",
      "\u001b[34mcreating index...\u001b[0m\n",
      "\u001b[34mindex created!\u001b[0m\n",
      "\u001b[34mRunning per image evaluation...\u001b[0m\n",
      "\u001b[34mEvaluate annotation type *bbox*\u001b[0m\n",
      "\u001b[34mDONE (t=5.71s).\u001b[0m\n",
      "\u001b[34mAccumulating evaluation results...\u001b[0m\n",
      "\u001b[34mDONE (t=0.18s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.061\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.132\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.053\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.213\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.388\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.021\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.075\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.108\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.378\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.525\u001b[0m\n",
      "\u001b[34m==EXPORTING THE MODEL==\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:459: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mback_prop=False is deprecated. Consider using tf.stop_gradient instead.\u001b[0m\n",
      "\u001b[34mInstead of:\u001b[0m\n",
      "\u001b[34mresults = tf.map_fn(fn, elems, back_prop=False)\u001b[0m\n",
      "\u001b[34mUse:\u001b[0m\n",
      "\u001b[34mresults = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\u001b[0m\n",
      "\u001b[34mW1002 01:54:28.965064 139683897325376 deprecation.py:641] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:459: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mback_prop=False is deprecated. Consider using tf.stop_gradient instead.\u001b[0m\n",
      "\u001b[34mInstead of:\u001b[0m\n",
      "\u001b[34mresults = tf.map_fn(fn, elems, back_prop=False)\u001b[0m\n",
      "\u001b[34mUse:\u001b[0m\n",
      "\u001b[34mresults = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\u001b[0m\n",
      "\u001b[34mI1002 01:54:32.421181 139683897325376 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mI1002 01:54:49.378740 139683897325376 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mI1002 01:55:02.346754 139683897325376 signature_serialization.py:148] Function `call_func` contains input name(s) resource with unsupported characters which will be renamed to weightsharedconvolutionalboxpredictor_predictiontower_conv2d_3_batchnorm_feature_4_fusedbatchnormv3_readvariableop_1_resource in the SavedModel.\u001b[0m\n",
      "\u001b[34mI1002 01:55:03.716419 139683897325376 api.py:460] feature_map_spatial_dims: [(80, 80), (40, 40), (20, 20), (10, 10), (5, 5)]\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7f09fc6fde80>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.736053 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7f09fc6fde80>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f09fc07fa00>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.990101 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f09fc07fa00>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f09b426e370>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.990265 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f09b426e370>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f09b426e6a0>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.990355 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f09b426e6a0>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f09b40e36d0>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.990472 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f09b40e36d0>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f09b40e3550>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.990644 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f09b40e3550>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f09b40e38b0>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.990756 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f09b40e38b0>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f09b4136a00>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.990850 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f09b4136a00>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f09b4136640>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.990946 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f09b4136640>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f09b4136f10>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.991039 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f09b4136f10>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f09b4147730>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.991138 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.convolutional.separable_conv2d.SeparableConv2D object at 0x7f09b4147730>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f09b4147550>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.991204 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f09b4147550>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f09b41478b0>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.991273 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f09b41478b0>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f09b423d220>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.991338 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f09b423d220>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f09b40fce50>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.991456 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f09b40fce50>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c5cef40>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.991551 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c5cef40>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c5ce220>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.991640 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c5ce220>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c5ce610>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.991727 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c5ce610>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c5cec10>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.991815 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c5cec10>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c3cbd00>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.991933 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c3cbd00>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c3cbc40>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.992063 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c3cbc40>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c5c8760>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.992162 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c5c8760>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f09b45bfee0>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.992288 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f09b45bfee0>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c3bd1f0>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.992372 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c3bd1f0>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c3bd580>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.992495 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c3bd580>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c3bd730>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.992586 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c3bd730>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c3bdb20>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.992673 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c3bdb20>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c3bd910>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.992747 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c3bd910>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c3bdf70>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.992835 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c3bdf70>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f09b41f46a0>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.992921 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f09b41f46a0>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c5ccd90>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.993012 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c5ccd90>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c5ccbb0>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.993105 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c5ccbb0>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c5ccc70>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.993184 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c5ccc70>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f09b40f0160>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.993272 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f09b40f0160>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f09b40f0e20>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.993373 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f09b40f0e20>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f09b40f03d0>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.993451 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f09b40f03d0>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f09b40f0580>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.993529 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f09b40f0580>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f09b4050f40>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.993622 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f09b4050f40>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f09b4244a90>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.993693 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f09b4244a90>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c59e310>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.993789 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c59e310>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c59eeb0>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.993866 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c59eeb0>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c59e6a0>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.993965 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c59e6a0>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c59eee0>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.994039 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c59eee0>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c5d8520>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.994126 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <object_detection.core.freezable_batch_norm.FreezableBatchNorm object at 0x7f099c5d8520>, because it is not built.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c5d8940>, because it is not built.\u001b[0m\n",
      "\u001b[34mW1002 01:55:14.994212 139683897325376 save_impl.py:66] Skipping full serialization of Keras layer <keras.src.layers.core.lambda_layer.Lambda object at 0x7f099c5d8940>, because it is not built.\u001b[0m\n",
      "\u001b[34mI1002 01:55:26.381936 139683897325376 save.py:274] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalClassHead_layer_call_fn while saving (showing 5 of 173). These functions will not be directly callable after loading.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /tmp/exported/saved_model/assets\u001b[0m\n",
      "\u001b[34mI1002 01:55:46.980482 139683897325376 builder_impl.py:804] Assets written to: /tmp/exported/saved_model/assets\u001b[0m\n",
      "\u001b[34mI1002 01:55:57.032479 139683897325376 fingerprinting_utils.py:48] Writing fingerprint to /tmp/exported/saved_model/fingerprint.pb\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Writing pipeline config file to /tmp/exported/pipeline.config\u001b[0m\n",
      "\u001b[34mI1002 01:55:57.577199 139683897325376 config_util.py:253] Writing pipeline config file to /tmp/exported/pipeline.config\u001b[0m\n",
      "\u001b[34m2025-10-02 01:55:59,430 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2025-10-02 01:56:54 Uploading - Uploading generated training model\n",
      "2025-10-02 01:56:54 Completed - Training job completed\n",
      "Training seconds: 1218\n",
      "Billable seconds: 1218\n"
     ]
    }
   ],
   "source": [
    "tensorboard_output_config = sagemaker.debugger.TensorBoardOutputConfig(\n",
    "    s3_output_path=tensorboard_s3_prefix,\n",
    "    container_local_output_path='/opt/training/'\n",
    ")\n",
    "\n",
    "estimator = CustomFramework(\n",
    "    role=role,\n",
    "    image_uri=container,\n",
    "    entry_point='run_training.sh',\n",
    "    source_dir='source_dir/',\n",
    "    hyperparameters={\n",
    "        \"model_dir\": \"/opt/training\",        \n",
    "        \"pipeline_config_path\": \"pipeline.config\",\n",
    "        \"num_train_steps\": \"2000\",    \n",
    "        \"sample_1_of_n_eval_examples\": \"1\"\n",
    "    },\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g5.xlarge',\n",
    "    tensorboard_output_config=tensorboard_output_config,\n",
    "    disable_profiler=True,\n",
    "    base_job_name='tf2-object-detection'\n",
    ")\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84545881",
   "metadata": {},
   "source": [
    "You should be able to see your model training in the AWS webapp as shown below:\n",
    "![ECR Example](../data/example_trainings.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9844f25",
   "metadata": {},
   "source": [
    "## Improve on the initial model\n",
    "\n",
    "Most likely, this initial experiment did not yield optimal results. However, you can make multiple changes to the `pipeline.config` file to improve this model. One obvious change consists in improving the data augmentation strategy. The [`preprocessor.proto`](https://github.com/tensorflow/models/blob/master/research/object_detection/protos/preprocessor.proto) file contains the different data augmentation method available in the Tf Object Detection API. Justify your choices of augmentations in the write-up.\n",
    "\n",
    "Keep in mind that the following are also available:\n",
    "* experiment with the optimizer: type of optimizer, learning rate, scheduler etc\n",
    "* experiment with the architecture. The Tf Object Detection API model zoo offers many architectures. Keep in mind that the pipeline.config file is unique for each architecture and you will have to edit it.\n",
    "* visualize results on the test frames using the `2_deploy_model` notebook available in this repository.\n",
    "\n",
    "In the cell below, write down all the different approaches you have experimented with, why you have chosen them and what you would have done if you had more time and resources. Justify your choices using the tensorboard visualizations (take screenshots and insert them in your write-up), the metrics on the evaluation set and the generated animation you have created with [this tool](../2_run_inference/2_deploy_model.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17284a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your write-up goes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba47930a-21a8-4de2-a20d-4ae01f9d213a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
